{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicegorge/INF554/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../AJA')\n",
    "import AJA as aja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données \n",
    "df_train_nodes, df_train_edges, df_test_nodes, df_test_edges = aja.get_data()\n",
    "\n",
    "# feature extraction\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# node\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# sentence length normalized\n",
    "df_train_nodes['sentence_length'] = df_train_nodes['text'].apply(lambda s: len(s.split()))\n",
    "df_train_nodes['sentence_length'] = scaler.fit_transform(df_train_nodes['sentence_length'].values.reshape(-1, 1))\n",
    "df_test_nodes['sentence_length'] = df_test_nodes['text'].apply(lambda s: len(s.split()))\n",
    "df_test_nodes['sentence_length'] = scaler.transform(df_test_nodes['sentence_length'].values.reshape(-1, 1))\n",
    "\n",
    "df_train_nodes['nb_occurences'] = df_train_nodes['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_train_nodes['nb_occurences'] = scaler.fit_transform(df_train_nodes['nb_occurences'].values.reshape(-1, 1))\n",
    "df_test_nodes['nb_occurences'] = df_test_nodes['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_test_nodes['nb_occurences'] = scaler.transform(df_test_nodes['nb_occurences'].values.reshape(-1, 1))\n",
    "\n",
    "df_train_nodes['nb_words_more_7'] = df_train_nodes['text'].apply(lambda x: sum(len(mot) > 7 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_train_nodes['nb_words_more_7'] = scaler.fit_transform(df_train_nodes['nb_words_more_7'].values.reshape(-1, 1))\n",
    "df_test_nodes['nb_words_more_7'] = df_test_nodes['text'].apply(lambda x: sum(len(mot) > 7 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_test_nodes['nb_words_more_7'] = scaler.transform(df_test_nodes['nb_words_more_7'].values.reshape(-1, 1))\n",
    "\n",
    "# speaker hot-one encoding\n",
    "one_hot_encoded = pd.get_dummies(df_train_nodes['speaker_int'], prefix='speaker', dtype=int)\n",
    "df_train_nodes = df_train_nodes.drop('speaker_int', axis=1)\n",
    "df_train_nodes = df_train_nodes.drop('speaker_text', axis=1)\n",
    "df_train_nodes = pd.concat([df_train_nodes, one_hot_encoded], axis=1)\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(df_test_nodes['speaker_int'], prefix='speaker', dtype=int)\n",
    "df_test_nodes = df_test_nodes.drop('speaker_int', axis=1)\n",
    "df_test_nodes = df_test_nodes.drop('speaker_text', axis=1)\n",
    "df_test_nodes = pd.concat([df_test_nodes, one_hot_encoded], axis=1)\n",
    "\n",
    "# TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train_nodes['text'])\n",
    "df_train_nodes['tfidf_sum'] = tfidf_matrix.sum(axis=1)\n",
    "df_train_nodes['tfidf_max'] = tfidf_matrix.max(axis=1).toarray().flatten()\n",
    "\n",
    "tfidf_matrix_test = tfidf_vectorizer.fit_transform(df_test_nodes['text'])\n",
    "df_test_nodes['tfidf_sum'] = tfidf_matrix_test.sum(axis=1)\n",
    "df_test_nodes['tfidf_max'] = tfidf_matrix_test.max(axis=1).toarray().flatten()\n",
    "\n",
    "df_train_nodes['tfidf_sum'] = scaler.fit_transform(df_train_nodes['tfidf_sum'].values.reshape(-1,1))\n",
    "df_test_nodes['tfidf_sum'] = scaler.transform(df_test_nodes['tfidf_sum'].values.reshape(-1,1))\n",
    "\n",
    "df_train_nodes['tfidf_max'] = scaler.fit_transform(df_train_nodes['tfidf_max'].values.reshape(-1,1))\n",
    "df_test_nodes['tfidf_max'] = scaler.transform(df_test_nodes['tfidf_max'].values.reshape(-1,1))\n",
    "\n",
    "df_train_nodes['yeah'] = df_train_nodes['text'].apply(lambda x: 1 if 'yeah' in x.lower() else 0)\n",
    "df_test_nodes['yeah'] = df_test_nodes['text'].apply(lambda x: 1 if 'yeah' in x.lower() else 0)\n",
    "\n",
    "\n",
    "# edge\n",
    "new_df = pd.DataFrame({\n",
    "        'transcription': df_train_edges['transcription'],\n",
    "        'start': df_train_edges['end'],\n",
    "        'end': df_train_edges['start'],\n",
    "        'type_int': 16 + df_train_edges['type_int'],\n",
    "        'type_text': df_train_edges['type_text'] + \"_reverse\"\n",
    "    })\n",
    "df_train_edges = pd.concat([df_train_edges, new_df], ignore_index=True)\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "        'transcription': df_test_edges['transcription'],\n",
    "        'start': df_test_edges['end'],\n",
    "        'end': df_test_edges['start'],\n",
    "        'type_int': 16 + df_test_edges['type_int'],\n",
    "        'type_text': df_test_edges['type_text'] + \"_reverse\"\n",
    "    })\n",
    "df_test_edges = pd.concat([df_test_edges, new_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des graphs\n",
    "train_graphs, test_graphs = aja.make_graphs(df_train_nodes, df_train_edges, df_test_nodes, df_test_edges)\n",
    "N_features = train_graphs['ES2002a'].x.shape[1]\n",
    "train_graphs, validation_graphs = aja.train_validation_split(train_graphs, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>line</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>bert_0</th>\n",
       "      <th>bert_1</th>\n",
       "      <th>bert_2</th>\n",
       "      <th>bert_3</th>\n",
       "      <th>bert_4</th>\n",
       "      <th>bert_5</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>nb_occurences</th>\n",
       "      <th>nb_words_more_7</th>\n",
       "      <th>speaker_0</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_2</th>\n",
       "      <th>speaker_3</th>\n",
       "      <th>tfidf_sum</th>\n",
       "      <th>tfidf_max</th>\n",
       "      <th>yeah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057809</td>\n",
       "      <td>-0.085828</td>\n",
       "      <td>-0.035720</td>\n",
       "      <td>-0.011185</td>\n",
       "      <td>0.062363</td>\n",
       "      <td>-0.023545</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008131</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.168929</td>\n",
       "      <td>1.439285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.054862</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>-0.035741</td>\n",
       "      <td>-0.051808</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008131</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.168929</td>\n",
       "      <td>1.439285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.054665</td>\n",
       "      <td>-0.073837</td>\n",
       "      <td>-0.017161</td>\n",
       "      <td>-0.064276</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.062475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789302</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>0.456915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.228067</td>\n",
       "      <td>-0.927421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>3</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>-0.072719</td>\n",
       "      <td>-0.017206</td>\n",
       "      <td>-0.088992</td>\n",
       "      <td>-0.048035</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.558773</td>\n",
       "      <td>1.365643</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.514793</td>\n",
       "      <td>0.709401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>4</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.028654</td>\n",
       "      <td>-0.015151</td>\n",
       "      <td>0.095910</td>\n",
       "      <td>-0.059113</td>\n",
       "      <td>0.042067</td>\n",
       "      <td>0.033088</td>\n",
       "      <td>...</td>\n",
       "      <td>1.088874</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.838146</td>\n",
       "      <td>-1.725435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>5</td>\n",
       "      <td>Um so first of all , just to kind of make sure...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.028386</td>\n",
       "      <td>-0.046021</td>\n",
       "      <td>0.023957</td>\n",
       "      <td>-0.064278</td>\n",
       "      <td>-0.006400</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>...</td>\n",
       "      <td>1.688019</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.729041</td>\n",
       "      <td>-1.117057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>6</td>\n",
       "      <td>I'm Laura and I'm the project manager .</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>-0.059705</td>\n",
       "      <td>0.051351</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>0.072220</td>\n",
       "      <td>-0.019084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.087169</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;vocalsound&gt; Do you want to introduce yourself...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.079113</td>\n",
       "      <td>-0.097972</td>\n",
       "      <td>0.070705</td>\n",
       "      <td>-0.016207</td>\n",
       "      <td>-0.016670</td>\n",
       "      <td>-0.007734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190158</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>1.561747</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585792</td>\n",
       "      <td>-0.726315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>8</td>\n",
       "      <td>Great .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.106075</td>\n",
       "      <td>-0.030617</td>\n",
       "      <td>-0.090078</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>0.066486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858345</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.168929</td>\n",
       "      <td>1.439285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>9</td>\n",
       "      <td>Hi , I'm David and I'm supposed to be an indus...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014584</td>\n",
       "      <td>0.074371</td>\n",
       "      <td>0.048543</td>\n",
       "      <td>-0.006655</td>\n",
       "      <td>-0.041892</td>\n",
       "      <td>-0.029181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789302</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>2.666580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.832774</td>\n",
       "      <td>-1.193867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>10</td>\n",
       "      <td>Okay .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.067186</td>\n",
       "      <td>-0.048921</td>\n",
       "      <td>-0.063839</td>\n",
       "      <td>-0.032437</td>\n",
       "      <td>0.080593</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858345</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.168929</td>\n",
       "      <td>1.439285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>11</td>\n",
       "      <td>And I'm Andrew and I'm uh our marketing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041912</td>\n",
       "      <td>-0.072649</td>\n",
       "      <td>0.018905</td>\n",
       "      <td>-0.076320</td>\n",
       "      <td>0.053862</td>\n",
       "      <td>-0.033870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>1.365643</td>\n",
       "      <td>0.456915</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051681</td>\n",
       "      <td>-0.223345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>12</td>\n",
       "      <td>Um I'm Craig and I'm User Interface .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.094336</td>\n",
       "      <td>-0.101444</td>\n",
       "      <td>-0.009792</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>-0.028230</td>\n",
       "      <td>-0.069749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>0.456915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>13</td>\n",
       "      <td>expert .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.042211</td>\n",
       "      <td>0.077820</td>\n",
       "      <td>-0.008508</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>-0.055671</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858345</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.168929</td>\n",
       "      <td>1.439285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>14</td>\n",
       "      <td>Great .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.106075</td>\n",
       "      <td>-0.030617</td>\n",
       "      <td>-0.090078</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>0.066486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.858345</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.168929</td>\n",
       "      <td>1.439285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>15</td>\n",
       "      <td>Okay . &lt;vocalsound&gt; Um</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005039</td>\n",
       "      <td>-0.098356</td>\n",
       "      <td>0.014405</td>\n",
       "      <td>-0.068420</td>\n",
       "      <td>-0.053244</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.558773</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.370581</td>\n",
       "      <td>-0.409491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>16</td>\n",
       "      <td>so we're designing a new remote control and um...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.074776</td>\n",
       "      <td>-0.029112</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>-0.013363</td>\n",
       "      <td>0.069836</td>\n",
       "      <td>-0.034852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339944</td>\n",
       "      <td>1.365643</td>\n",
       "      <td>1.561747</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.004427</td>\n",
       "      <td>-0.726592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>17</td>\n",
       "      <td>Oh I have to record who's here actually .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.071743</td>\n",
       "      <td>-0.064476</td>\n",
       "      <td>-0.003082</td>\n",
       "      <td>-0.043607</td>\n",
       "      <td>0.025314</td>\n",
       "      <td>0.044924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190158</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>0.456915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503413</td>\n",
       "      <td>-0.643581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>18</td>\n",
       "      <td>So that's David , Andrew and Craig , isn't it ?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043112</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.025064</td>\n",
       "      <td>-0.086010</td>\n",
       "      <td>-0.025405</td>\n",
       "      <td>0.038029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489730</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.512077</td>\n",
       "      <td>-0.924452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>19</td>\n",
       "      <td>And you all arrived on time .</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.010130</td>\n",
       "      <td>-0.011410</td>\n",
       "      <td>0.032920</td>\n",
       "      <td>0.040065</td>\n",
       "      <td>0.108141</td>\n",
       "      <td>-0.057655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109414</td>\n",
       "      <td>-0.368253</td>\n",
       "      <td>-0.647917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132666</td>\n",
       "      <td>0.219125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   transcription  line                                               text  \\\n",
       "0        ES2002a     0                                               Okay   \n",
       "1        ES2002a     1                                              Right   \n",
       "2        ES2002a     2  <vocalsound> Um well this is the kick-off meet...   \n",
       "3        ES2002a     3                             Um <vocalsound> and um   \n",
       "4        ES2002a     4  this is just what we're gonna be doing over th...   \n",
       "5        ES2002a     5  Um so first of all , just to kind of make sure...   \n",
       "6        ES2002a     6            I'm Laura and I'm the project manager .   \n",
       "7        ES2002a     7  <vocalsound> Do you want to introduce yourself...   \n",
       "8        ES2002a     8                                            Great .   \n",
       "9        ES2002a     9  Hi , I'm David and I'm supposed to be an indus...   \n",
       "10       ES2002a    10                                             Okay .   \n",
       "11       ES2002a    11            And I'm Andrew and I'm uh our marketing   \n",
       "12       ES2002a    12              Um I'm Craig and I'm User Interface .   \n",
       "13       ES2002a    13                                           expert .   \n",
       "14       ES2002a    14                                            Great .   \n",
       "15       ES2002a    15                             Okay . <vocalsound> Um   \n",
       "16       ES2002a    16  so we're designing a new remote control and um...   \n",
       "17       ES2002a    17          Oh I have to record who's here actually .   \n",
       "18       ES2002a    18    So that's David , Andrew and Craig , isn't it ?   \n",
       "19       ES2002a    19                      And you all arrived on time .   \n",
       "\n",
       "    label    bert_0    bert_1    bert_2    bert_3    bert_4    bert_5  ...  \\\n",
       "0       0 -0.057809 -0.085828 -0.035720 -0.011185  0.062363 -0.023545  ...   \n",
       "1       0 -0.054862  0.047607 -0.032626 -0.010949 -0.035741 -0.051808  ...   \n",
       "2       1 -0.054665 -0.073837 -0.017161 -0.064276  0.004937  0.062475  ...   \n",
       "3       0 -0.010416 -0.072719 -0.017206 -0.088992 -0.048035  0.051155  ...   \n",
       "4       0 -0.028654 -0.015151  0.095910 -0.059113  0.042067  0.033088  ...   \n",
       "5       0 -0.028386 -0.046021  0.023957 -0.064278 -0.006400 -0.002545  ...   \n",
       "6       0  0.015530 -0.059705  0.051351  0.020685  0.072220 -0.019084  ...   \n",
       "7       0 -0.079113 -0.097972  0.070705 -0.016207 -0.016670 -0.007734  ...   \n",
       "8       0 -0.106075 -0.030617 -0.090078  0.004343  0.043766  0.066486  ...   \n",
       "9       0 -0.014584  0.074371  0.048543 -0.006655 -0.041892 -0.029181  ...   \n",
       "10      0 -0.067186 -0.048921 -0.063839 -0.032437  0.080593  0.039625  ...   \n",
       "11      0  0.041912 -0.072649  0.018905 -0.076320  0.053862 -0.033870  ...   \n",
       "12      0 -0.094336 -0.101444 -0.009792  0.014747 -0.028230 -0.069749  ...   \n",
       "13      0 -0.042211  0.077820 -0.008508  0.063012 -0.055671 -0.020211  ...   \n",
       "14      0 -0.106075 -0.030617 -0.090078  0.004343  0.043766  0.066486  ...   \n",
       "15      0 -0.005039 -0.098356  0.014405 -0.068420 -0.053244  0.041328  ...   \n",
       "16      1 -0.074776 -0.029112  0.013208 -0.013363  0.069836 -0.034852  ...   \n",
       "17      0 -0.071743 -0.064476 -0.003082 -0.043607  0.025314  0.044924  ...   \n",
       "18      0  0.043112 -0.026713 -0.025064 -0.086010 -0.025405  0.038029  ...   \n",
       "19      0 -0.010130 -0.011410  0.032920  0.040065  0.108141 -0.057655  ...   \n",
       "\n",
       "    sentence_length  nb_occurences  nb_words_more_7  speaker_0  speaker_1  \\\n",
       "0         -1.008131      -0.368253        -0.647917          1          0   \n",
       "1         -1.008131      -0.368253        -0.647917          1          0   \n",
       "2          0.789302      -0.368253         0.456915          1          0   \n",
       "3         -0.558773       1.365643        -0.647917          1          0   \n",
       "4          1.088874      -0.368253        -0.647917          1          0   \n",
       "5          1.688019      -0.368253        -0.647917          1          0   \n",
       "6          0.040372      -0.368253        -0.647917          1          0   \n",
       "7          0.190158      -0.368253         1.561747          1          0   \n",
       "8         -0.858345      -0.368253        -0.647917          0          1   \n",
       "9          0.789302      -0.368253         2.666580          0          0   \n",
       "10        -0.858345      -0.368253        -0.647917          1          0   \n",
       "11         0.040372       1.365643         0.456915          0          1   \n",
       "12         0.040372      -0.368253         0.456915          0          0   \n",
       "13        -0.858345      -0.368253        -0.647917          0          1   \n",
       "14        -0.858345      -0.368253        -0.647917          1          0   \n",
       "15        -0.558773      -0.368253        -0.647917          1          0   \n",
       "16         0.339944       1.365643         1.561747          1          0   \n",
       "17         0.190158      -0.368253         0.456915          1          0   \n",
       "18         0.489730      -0.368253        -0.647917          1          0   \n",
       "19        -0.109414      -0.368253        -0.647917          1          0   \n",
       "\n",
       "    speaker_2  speaker_3  tfidf_sum  tfidf_max  yeah  \n",
       "0           0          0  -1.168929   1.439285     0  \n",
       "1           0          0  -1.168929   1.439285     0  \n",
       "2           0          0   1.228067  -0.927421     0  \n",
       "3           0          0  -0.514793   0.709401     0  \n",
       "4           0          0   1.838146  -1.725435     0  \n",
       "5           0          0   1.729041  -1.117057     0  \n",
       "6           0          0  -0.087169   0.102399     0  \n",
       "7           0          0   0.585792  -0.726315     0  \n",
       "8           0          0  -1.168929   1.439285     0  \n",
       "9           1          0   0.832774  -1.193867     0  \n",
       "10          0          0  -1.168929   1.439285     0  \n",
       "11          0          0   0.051681  -0.223345     0  \n",
       "12          0          1   0.004783   0.000793     0  \n",
       "13          0          0  -1.168929   1.439285     0  \n",
       "14          0          0  -1.168929   1.439285     0  \n",
       "15          0          0  -0.370581  -0.409491     0  \n",
       "16          0          0   1.004427  -0.726592     0  \n",
       "17          0          0   0.503413  -0.643581     0  \n",
       "18          0          0   0.512077  -0.924452     0  \n",
       "19          0          0   0.132666   0.219125     0  \n",
       "\n",
       "[20 rows x 398 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_nodes.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelsGCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, input_dim, post_conv_dim, output_dim, identity=False):\n",
    "        super(MultiChannelsGCN, self).__init__()\n",
    "        self.identity = identity\n",
    "        self.channels = channels\n",
    "        self.input_dim = input_dim\n",
    "        self.post_conv_dim = post_conv_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.GCN = nn.ModuleList([GCNConv(input_dim, post_conv_dim) for _ in range(channels)])\n",
    "        if identity:\n",
    "            self.dense = nn.Linear(post_conv_dim * (channels + 1), output_dim)\n",
    "            self.denseID = nn.Linear(input_dim, post_conv_dim)\n",
    "        else:\n",
    "            self.dense = nn.Linear(post_conv_dim * channels, output_dim)\n",
    "\n",
    "    def forward(self, nodes, edges):\n",
    "        X = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.post_conv_dim)\n",
    "            else:\n",
    "                x = F.relu(self.GCN[k](nodes, edges[k]))\n",
    "            X.append(x)\n",
    "        if self.identity:\n",
    "            X.append(F.relu(self.denseID(nodes)))\n",
    "        concat = torch.cat(X, dim=1)\n",
    "        return F.relu(self.dense(concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définie son plus beau modèle\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.GCN1 = MultiChannelsGCN(channels, input_dim, 50, 20, identity=True)\n",
    "        #self.dropout = nn.Dropout(0.3)  \n",
    "        self.dense1 = nn.Linear(20,1)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "        x = self.GCN1(nodes, edges)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.dense1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, graph):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(graph)\n",
    "        return np.array((logits > 0.5).int()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class AjaPyTorchWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, train_graph_dict, validation_graph_dict, verbose=1, max_epochs=10):\n",
    "        # Training logic using your PyTorch model\n",
    "        # ...\n",
    "                \n",
    "        # Move the model and data to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "        # Use DataLoader to create batches of data\n",
    "        train_loader = DataLoader(list(train_graph_dict.values()), batch_size=1, shuffle=True)\n",
    "        N_train = len(train_loader)\n",
    "        validation_loader = DataLoader(list(validation_graph_dict.values()), batch_size=1, shuffle=False)\n",
    "        N_validation = len(validation_loader)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('Training on', N_train, 'graphs, validating on', N_validation, 'graphs')\n",
    "\n",
    "        # Train the model\n",
    "        model_name = \"model_py_torch\"\n",
    "        best_f1_score = 0\n",
    "        for epoch in range(max_epochs):\n",
    "            if verbose > 0:\n",
    "                print('- Epoch', f'{epoch + 1:03d}', '-')\n",
    "            # training\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data).squeeze()\n",
    "                loss = self.criterion(output, data.y.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            average_loss = total_loss / N_train\n",
    "            if verbose > 1:\n",
    "                print('Loss:', f'{average_loss:.4f}')\n",
    "            \n",
    "\n",
    "            # Evaluate the model on the training set\n",
    "            self.model.eval()\n",
    "            f1_moyen_train = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                y_pred = self.model.predict(data)\n",
    "                y_true = data.y.cpu().numpy()\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                f1_moyen_train += f1\n",
    "            f1_moyen_train /= N_train\n",
    "            if verbose > 1:\n",
    "                print('F1 train:', f1_moyen_train)\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            self.model.eval()\n",
    "            f1_moyen_valid = 0\n",
    "            for data in validation_loader:\n",
    "                data = data.to(device)\n",
    "                y_pred = self.model.predict(data)\n",
    "                y_true = data.y.cpu().numpy()\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                f1_moyen_valid += f1\n",
    "            f1_moyen_valid /= N_validation\n",
    "            if verbose > 1:\n",
    "                print('F1 valid:', f1_moyen_valid)\n",
    "\n",
    "            # callbacks ou autre\n",
    "            if f1_moyen_valid > best_f1_score:\n",
    "                if verbose > 1:\n",
    "                    print('It\\'s better !' )\n",
    "                torch.save(self.model.state_dict(), \"training_states/\" + model_name + \"-best.pth\")\n",
    "            else:\n",
    "                self.optimizer.param_groups[0]['lr'] /= 2\n",
    "                if verbose > 1:\n",
    "                    print('Learning rate reduced to:', self.optimizer.param_groups[0]['lr'])\n",
    "            if verbose > 1:\n",
    "                print('')\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print('Training finished !')\n",
    "\n",
    "        self.model.load_state_dict(torch.load(\"training_states/\" + model_name + \"-best.pth\"))\n",
    "\n",
    "    def predict(self, graphs_dict):\n",
    "        # Prediction logic using your PyTorch model\n",
    "        # ...\n",
    "        self.model.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        result = {}\n",
    "        for key, graph in graphs_dict.items():\n",
    "            data = graph.to(device)\n",
    "            y_pred = self.model.predict(data)\n",
    "            result[key] = y_pred\n",
    "        return result\n",
    "\n",
    "\n",
    "    def score(self, graphs_dict):\n",
    "        # Scoring logic using your PyTorch model\n",
    "        # ...\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        validation_loader = DataLoader(list(graphs_dict.values()), batch_size=1, shuffle=False)\n",
    "        N_validation = len(validation_loader)\n",
    "        self.model.eval()\n",
    "        f1_moyen_valid = 0\n",
    "        for data in validation_loader:\n",
    "            data = data.to(device)\n",
    "            y_pred = self.model.predict(data)\n",
    "            y_true = data.y.cpu().numpy()\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            f1_moyen_valid += f1\n",
    "        f1_moyen_valid /= N_validation\n",
    "        return f1_moyen_valid\n",
    "    \n",
    "    def predict_proba(self, graphs_dict):\n",
    "        # Prediction logic using your PyTorch model\n",
    "        # ...\n",
    "        self.model.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        result = {}\n",
    "        for key, graph in graphs_dict.items():\n",
    "            data = graph.to(device)\n",
    "            output = self.model(data).squeeze()\n",
    "            # Ajouter une fonction d'activation (softmax, sigmoïde, etc.) selon votre modèle\n",
    "            probabilities = torch.softmax(output, dim=-1)\n",
    "            result[key] = probabilities.cpu().detach().numpy()\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_moyen(pred_dict, true_graphs_dict):\n",
    "    f1_moyen = 0\n",
    "    for key, pred in pred_dict.items():\n",
    "        y_true = true_graphs_dict[key].y.numpy()\n",
    "        f1_moyen += f1_score(y_true, pred)\n",
    "    f1_moyen /= len(pred_dict)\n",
    "    return f1_moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 78 graphs, validating on 19 graphs\n",
      "- Epoch 001 -\n",
      "Loss: 1.0914\n",
      "F1 train: 0.556367328343316\n",
      "F1 valid: 0.5731097011052926\n",
      "It's better !\n",
      "\n",
      "- Epoch 002 -\n",
      "Loss: 1.0677\n",
      "F1 train: 0.5732777806549613\n",
      "F1 valid: 0.5816558826246276\n",
      "It's better !\n",
      "\n",
      "- Epoch 003 -\n",
      "Loss: 1.0606\n",
      "F1 train: 0.5719429191063462\n",
      "F1 valid: 0.5810750006328472\n",
      "It's better !\n",
      "\n",
      "- Epoch 004 -\n",
      "Loss: 1.0548\n",
      "F1 train: 0.597146585157545\n",
      "F1 valid: 0.5947773616474321\n",
      "It's better !\n",
      "\n",
      "- Epoch 005 -\n",
      "Loss: 1.0494\n",
      "F1 train: 0.6050428200579929\n",
      "F1 valid: 0.5954199027780251\n",
      "It's better !\n",
      "\n",
      "- Epoch 006 -\n",
      "Loss: 1.0441\n",
      "F1 train: 0.6130855106725387\n",
      "F1 valid: 0.5970201378144628\n",
      "It's better !\n",
      "\n",
      "- Epoch 007 -\n",
      "Loss: 1.0379\n",
      "F1 train: 0.6273025746205838\n",
      "F1 valid: 0.6008671292580287\n",
      "It's better !\n",
      "\n",
      "- Epoch 008 -\n",
      "Loss: 1.0331\n",
      "F1 train: 0.6379921709749488\n",
      "F1 valid: 0.5962404102914051\n",
      "It's better !\n",
      "\n",
      "- Epoch 009 -\n",
      "Loss: 1.0263\n",
      "F1 train: 0.6551096707680432\n",
      "F1 valid: 0.6005420673174923\n",
      "It's better !\n",
      "\n",
      "- Epoch 010 -\n",
      "Loss: 1.0216\n",
      "F1 train: 0.6213508652823464\n",
      "F1 valid: 0.5885194059662405\n",
      "It's better !\n",
      "\n",
      "- Epoch 011 -\n",
      "Loss: 1.0193\n",
      "F1 train: 0.6689475686920847\n",
      "F1 valid: 0.6024055463026164\n",
      "It's better !\n",
      "\n",
      "- Epoch 012 -\n",
      "Loss: 1.0098\n",
      "F1 train: 0.6812829829787346\n",
      "F1 valid: 0.6008417775287301\n",
      "It's better !\n",
      "\n",
      "- Epoch 013 -\n",
      "Loss: 1.0044\n",
      "F1 train: 0.6861361967923589\n",
      "F1 valid: 0.6014740865091475\n",
      "It's better !\n",
      "\n",
      "- Epoch 014 -\n",
      "Loss: 0.9990\n",
      "F1 train: 0.7004288297726449\n",
      "F1 valid: 0.6027910126833116\n",
      "It's better !\n",
      "\n",
      "- Epoch 015 -\n",
      "Loss: 0.9974\n",
      "F1 train: 0.6494519896547434\n",
      "F1 valid: 0.5891535246036511\n",
      "It's better !\n",
      "\n",
      "- Epoch 016 -\n",
      "Loss: 0.9961\n",
      "F1 train: 0.7103244765618505\n",
      "F1 valid: 0.6022139593417853\n",
      "It's better !\n",
      "\n",
      "- Epoch 017 -\n",
      "Loss: 0.9913\n",
      "F1 train: 0.7168323965221044\n",
      "F1 valid: 0.6033610905188608\n",
      "It's better !\n",
      "\n",
      "- Epoch 018 -\n",
      "Loss: 0.9867\n",
      "F1 train: 0.6893089573080812\n",
      "F1 valid: 0.598262283095682\n",
      "It's better !\n",
      "\n",
      "- Epoch 019 -\n",
      "Loss: 0.9880\n",
      "F1 train: 0.7251634736583952\n",
      "F1 valid: 0.6060768675953037\n",
      "It's better !\n",
      "\n",
      "- Epoch 020 -\n",
      "Loss: 0.9805\n",
      "F1 train: 0.7435493777108281\n",
      "F1 valid: 0.601825975708497\n",
      "It's better !\n",
      "\n",
      "Training finished !\n",
      "0.601825975708497\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of your PyTorch model\n",
    "pytorch_model = NodeClassifier(32, N_features)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "alpha = 0.15730642604852357\n",
    "lr = 0.002272131994333311\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((1 - alpha) / alpha), reduction='mean')\n",
    "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=lr)\n",
    "\n",
    "# Create an instance of the custom wrapper\n",
    "model = AjaPyTorchWrapper(pytorch_model, criterion, optimizer)\n",
    "\n",
    "# Fit, predict, and score using scikit-learn-like API\n",
    "model.fit(train_graphs, validation_graphs, max_epochs=20,verbose=2)\n",
    "y_pred = model.predict(test_graphs)\n",
    "print(model.score(validation_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_bagging_models(n_bagging, train_graphs):\n",
    "    models = []\n",
    "    for i in range(n_bagging):\n",
    "    \n",
    "        print('Bagging', i+1)\n",
    "        pytorch_model = NodeClassifier(32, N_features)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((1 - alpha) / alpha), reduction='mean')\n",
    "        optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=0.01)\n",
    "        model = AjaPyTorchWrapper(pytorch_model, criterion, optimizer)\n",
    "        keys = list(train_graphs.keys())\n",
    "        bagging_train_graphs = {}\n",
    "        bagging_validation_graphs = {}\n",
    "        samples = random.choices(keys, k=len(keys))\n",
    "        c_train = 0\n",
    "        c_validation = 0\n",
    "        for key in keys:\n",
    "            if key in samples:\n",
    "                bagging_train_graphs[c_train] = train_graphs[key]\n",
    "                c_train += 1\n",
    "            else:\n",
    "                bagging_validation_graphs[c_validation] = train_graphs[key]\n",
    "                c_validation += 1\n",
    "        model.fit(bagging_train_graphs, bagging_validation_graphs, max_epochs=6, verbose=0)\n",
    "        models.append(model)\n",
    "        print('F1 score:', model.score(bagging_validation_graphs))\n",
    "    return models\n",
    "\n",
    "def predict_bagging(models, graphs_dict):\n",
    "    result = {}\n",
    "    for key, graph in graphs_dict.items():\n",
    "        y_pred = 0\n",
    "        for model in models:\n",
    "            y_pred += model.predict({key: graph})[key]\n",
    "        y_pred =  y_pred / len(models)\n",
    "        y_pred = np.array((y_pred > 0.5).astype(int)).flatten()\n",
    "        result[key] = y_pred\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging 1\n",
      "F1 score: 0.6004894427974068\n",
      "Bagging 2\n",
      "F1 score: 0.5405661662038173\n",
      "Bagging 3\n",
      "F1 score: 0.5913789909886713\n",
      "Bagging 4\n",
      "F1 score: 0.5797480448358324\n",
      "Bagging 5\n",
      "F1 score: 0.5891011636575166\n",
      "Bagging 6\n",
      "F1 score: 0.5985572465862873\n",
      "Bagging 7\n",
      "F1 score: 0.5725509234428762\n",
      "Bagging 8\n",
      "F1 score: 0.5185109392906673\n",
      "Bagging 9\n",
      "F1 score: 0.5533477511707815\n",
      "Bagging 10\n",
      "F1 score: 0.5494165920520147\n"
     ]
    }
   ],
   "source": [
    "models = get_bagging_models(10, {**train_graphs, **validation_graphs})\n",
    "test_prediction = predict_bagging(models, test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "aja.make_test_csv_submission_from_dict(test_prediction, 'bagging_alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
