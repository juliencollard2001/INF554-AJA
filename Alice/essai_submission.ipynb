{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lire les données de transcription\n",
    "def read_transcription(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Fonction pour lire les données du graphe de discours\n",
    "def read_discourse_graph(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [line.strip().split() for line in file]\n",
    "\n",
    "    # Convertir les valeurs non numériques en indices numériques\n",
    "    data = [(int(start), relation, int(end)) if start.isdigit() and end.isdigit() else (start, relation, end) for start, relation, end in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict):\n",
    "    rows = []\n",
    "\n",
    "      # Iterate through all edges in the discourse graph\n",
    "    for edge in discourse_graph:\n",
    "        index_start, relation_type, index_end = edge\n",
    "\n",
    "        # Retrieve speaker information\n",
    "        speaker = transcription[index_start]['speaker']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        speaker_id = speaker_dict.get(speaker, -1)\n",
    "\n",
    "        # Retrieve the sentence\n",
    "        text = transcription[index_start]['text']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        relation_type_id = relation_dict.get(relation_type, -1)\n",
    "\n",
    "        # Add a row to the DataFrame\n",
    "        rows.append({\n",
    "            'dialogue_id': dialogue_id,\n",
    "            'index_start': index_start,\n",
    "            'text': text,\n",
    "            'index_end': index_end,\n",
    "            'speaker_type': speaker_id,\n",
    "            'speaker_text': speaker,\n",
    "            'relation_type': relation_type_id,\n",
    "            'relation_text': relation_type\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des relations\n",
    "def create_relation_dict(discourse_graph):\n",
    "    relation_set = set()\n",
    "\n",
    "    # Collecter toutes les relations uniques\n",
    "    for edge in discourse_graph:\n",
    "        relation_set.add(edge[1])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    relation_dict = {relation: idx for idx, relation in enumerate(relation_set)}\n",
    "\n",
    "    return relation_dict\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des speakers\n",
    "def create_speaker_dict(transcription):\n",
    "    speaker_set = set()\n",
    "\n",
    "    # Collecter tous les locuteurs uniques\n",
    "    for utterance in transcription:\n",
    "        speaker_set.add(utterance['speaker'])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    speaker_dict = {speaker: idx for idx, speaker in enumerate(speaker_set)}\n",
    "\n",
    "    return speaker_dict\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "# Function to get labels for a dialogue\n",
    "def get_label(dialogue_id, index,labels_data):\n",
    "    return labels_data.get(dialogue_id, [])[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacez 'votre_chemin' par le chemin correct\n",
    "path_train= Path(\"data/training\")\n",
    "path_test= Path(\"data/test\")\n",
    "\n",
    "# Remplacez 'vos_dialogue_ids' par votre liste réelle d'identifiants de dialogue\n",
    "dialogue_ids = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "dialogue_ids = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids])\n",
    "dialogue_ids.remove('IS1002a')\n",
    "dialogue_ids.remove('IS1005d')\n",
    "dialogue_ids.remove('TS3012c')\n",
    "\n",
    "dialogue_ids_test = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "dialogue_ids_test = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids_test])\n",
    "\n",
    "# Liste pour stocker les DataFrames de chaque dialogue\n",
    "dfs = []\n",
    "dfs_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_train / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_train / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs.append(df)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': 0,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': 0,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs.append(pd.DataFrame([last_row]))\n",
    "\n",
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_test / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_test / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df_test = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs_test.append(df_test)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': 0,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': 0,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs_test.append(pd.DataFrame([last_row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72623 entries, 0 to 72622\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   dialogue_id    72623 non-null  object\n",
      " 1   index_start    72623 non-null  int64 \n",
      " 2   text           72623 non-null  object\n",
      " 3   index_end      72623 non-null  int64 \n",
      " 4   speaker_type   72623 non-null  int64 \n",
      " 5   speaker_text   72623 non-null  object\n",
      " 6   relation_type  72623 non-null  int64 \n",
      " 7   relation_text  72526 non-null  object\n",
      " 8   label          72623 non-null  int64 \n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 5.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>index_start</th>\n",
       "      <th>text</th>\n",
       "      <th>index_end</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>relation_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>1</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>Right</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>1</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>12</td>\n",
       "      <td>Explanation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>3</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>5</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>4</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>1</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id  index_start                                               text  \\\n",
       "0     ES2002a            0                                               Okay   \n",
       "1     ES2002a            1                                              Right   \n",
       "2     ES2002a            2  <vocalsound> Um well this is the kick-off meet...   \n",
       "3     ES2002a            3                             Um <vocalsound> and um   \n",
       "4     ES2002a            4  this is just what we're gonna be doing over th...   \n",
       "\n",
       "   index_end  speaker_type speaker_text  relation_type relation_text  label  \n",
       "0          1             2           PM              1  Continuation      0  \n",
       "1          2             2           PM              1  Continuation      0  \n",
       "2          3             2           PM             12   Explanation      1  \n",
       "3          4             2           PM              5   Elaboration      0  \n",
       "4          5             2           PM              1  Continuation      0  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concaténer tous les DataFrames en un seul\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "with open(\"data/training_labels.json\", 'r') as file:\n",
    "    labels_data = json.load(file)\n",
    "\n",
    "df['label'] = df.apply(lambda row: get_label(row['dialogue_id'], row['index_start'], labels_data), axis=1)\n",
    "\n",
    "# Afficher le DataFrame final\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcription_id in dialogue_ids:\n",
    "    bert_array = np.load('training/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df[df['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array):\n",
    "        df.at[indices[idx-1], 'text'] = value\n",
    "\n",
    "for transcription_id in dialogue_ids_test:\n",
    "    bert_array_test = np.load('test/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df_test[df_test['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array_test):\n",
    "        df_test.at[indices[idx-1], 'text'] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'éléments dans chaque liste\n",
    "num_elements = len(df['text'].iloc[0])\n",
    "\n",
    "# Créez de nouvelles colonnes pour chaque élément dans la liste\n",
    "new_columns = [f'coord_{i}' for i in range(num_elements)]\n",
    "\n",
    "# Appliquez une fonction qui divise chaque liste en plusieurs colonnes\n",
    "new_text_columns = df['text'].apply(pd.Series)\n",
    "\n",
    "# Renommez les nouvelles colonnes avec les noms spécifiques\n",
    "new_text_columns.columns = new_columns\n",
    "\n",
    "# Concaténez les nouvelles colonnes avec le DataFrame existant\n",
    "df = pd.concat([df, new_text_columns], axis=1)\n",
    "\n",
    "# Supprimez la colonne 'text' originale si nécessaire\n",
    "df = df.drop(['text','speaker_text','relation_text' ], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nombre d'éléments dans chaque liste\n",
    "num_elements = len(df_test['text'].iloc[0])\n",
    "\n",
    "# Appliquez une fonction qui divise chaque liste en plusieurs colonnes\n",
    "new_text_columns_test = df_test['text'].apply(pd.Series)\n",
    "\n",
    "# Renommez les nouvelles colonnes avec les noms spécifiques\n",
    "new_text_columns_test.columns = new_columns\n",
    "\n",
    "# Concaténez les nouvelles colonnes avec le DataFrame existant\n",
    "df_test = pd.concat([df_test, new_text_columns_test], axis=1)\n",
    "\n",
    "# Supprimez la colonne 'text' originale si nécessaire\n",
    "df_test = df_test.drop(['text','speaker_text','relation_text' ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_index'] = df['index_end'] - df['index_start']\n",
    "df_test['diff_index'] = df_test['index_end'] - df_test['index_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes pour la matrice de corrélation\n",
    "selected_columns = ['index_start', 'index_end', 'speaker_type', 'relation_type', 'label', 'diff_index']\n",
    "\n",
    "# Créer un sous-dataframe avec les colonnes sélectionnées\n",
    "corr_df = df[selected_columns]\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "# Tracer la matrice de corrélation colorée\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "#plt.title('Matrice de Corrélation')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "# Supprimer les colonnes non nécessaires de X_train (dialogue_id, index_start, index_end, etc.)\n",
    "X = df.drop(['dialogue_id', 'label'], axis=1)\n",
    "\n",
    "# Sélectionner la colonne 'label' comme y_train\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "y_train_ohe = to_categorical(y_train, num_classes=2)\n",
    "y_test_ohe = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "y_ohe = to_categorical(y, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_31 (Dense)            (None, 128)               49920     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50178 (196.01 KB)\n",
      "Trainable params: 50178 (196.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Réseaux de neurones\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, Input, LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(389))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Définition de la fonction F1-score\n",
    "def f1_score_keras(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return K.mean(f1)\n",
    "\n",
    "# Utilisation de la fonction F1-score comme métrique\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy', f1_score_keras])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1271/1271 [==============================] - 2s 2ms/step - loss: 0.5569 - accuracy: 0.7558 - f1_score_keras: 0.4294 - val_loss: 0.5515 - val_accuracy: 0.7596 - val_f1_score_keras: 0.4305\n",
      "Epoch 2/3\n",
      "1271/1271 [==============================] - 2s 2ms/step - loss: 0.5564 - accuracy: 0.7558 - f1_score_keras: 0.4294 - val_loss: 0.5517 - val_accuracy: 0.7596 - val_f1_score_keras: 0.4305\n",
      "Epoch 3/3\n",
      "1271/1271 [==============================] - 2s 2ms/step - loss: 0.5571 - accuracy: 0.7558 - f1_score_keras: 0.4293 - val_loss: 0.5517 - val_accuracy: 0.7596 - val_f1_score_keras: 0.4305\n",
      "681/681 [==============================] - 1s 1ms/step - loss: 0.5558 - accuracy: 0.7560 - f1_score_keras: 0.4295\n",
      "0.42951035499572754\n"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(X_train, y_train_ohe, epochs=3, batch_size=32, validation_split=0.2)\n",
    "loss, accuracy, f1_score = model.evaluate(X_test, y_test_ohe)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - 1s 972us/step\n",
      "Nb zeros: 0\n",
      "F1-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print(\"Nb zeros:\", sum(y_pred_class))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1816/1816 [==============================] - 4s 2ms/step - loss: 0.5680 - accuracy: 0.7455 - f1_score_keras: 0.4260 - val_loss: 0.5085 - val_accuracy: 0.8003 - val_f1_score_keras: 0.4402\n",
      "Epoch 2/3\n",
      "1816/1816 [==============================] - 4s 2ms/step - loss: 0.5678 - accuracy: 0.7455 - f1_score_keras: 0.4259 - val_loss: 0.5083 - val_accuracy: 0.8003 - val_f1_score_keras: 0.4402\n",
      "Epoch 3/3\n",
      "1816/1816 [==============================] - 3s 2ms/step - loss: 0.5678 - accuracy: 0.7455 - f1_score_keras: 0.4260 - val_loss: 0.5078 - val_accuracy: 0.8003 - val_f1_score_keras: 0.4402\n",
      "970/970 [==============================] - 1s 922us/step\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y_ohe, epochs=3, batch_size=32, validation_split=0.2)\n",
    "test_pred = model.predict(df_test.drop(['dialogue_id'], axis=1))\n",
    "test_labels = np.argmax(test_pred,axis=1)\n",
    "print(\"Nb zeros:\", sum(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire pour stocker les prédictions\n",
    "test_labels_final = {dialogue_id: [] for dialogue_id in dialogue_ids_test}\n",
    "\n",
    "# Parcourir les lignes de df_test\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Obtenez les indices des lignes correspondant au dialogue_id\n",
    "    indices = df_test[df_test['dialogue_id'] == dialogue_id].index\n",
    "    # Ajouter les valeurs de test_label[index] au dictionnaire\n",
    "    test_labels_final[dialogue_id] = test_labels[indices].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_labels_text_mlp.json\", \"w\") as file:\n",
    "    json.dump(test_labels_final, file, indent=4)\n",
    "\n",
    "#python make_submission.py --json_path Alice/test_labels_text_mlp.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
