{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 15:44:48.989141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lire les données de transcription\n",
    "def read_transcription(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Fonction pour lire les données du graphe de discours\n",
    "def read_discourse_graph(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [line.strip().split() for line in file]\n",
    "\n",
    "    # Convertir les valeurs non numériques en indices numériques\n",
    "    data = [(int(start), relation, int(end)) if start.isdigit() and end.isdigit() else (start, relation, end) for start, relation, end in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict):\n",
    "    rows = []\n",
    "\n",
    "      # Iterate through all edges in the discourse graph\n",
    "    for edge in discourse_graph:\n",
    "        index_start, relation_type, index_end = edge\n",
    "\n",
    "        # Retrieve speaker information\n",
    "        speaker = transcription[index_start]['speaker']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        speaker_id = speaker_dict.get(speaker, -1)\n",
    "\n",
    "        # Retrieve the sentence\n",
    "        text = transcription[index_start]['text']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        relation_type_id = relation_dict.get(relation_type, -1)\n",
    "\n",
    "        # Add a row to the DataFrame\n",
    "        rows.append({\n",
    "            'dialogue_id': dialogue_id,\n",
    "            'index_start': index_start,\n",
    "            'text': text,\n",
    "            'index_end': index_end,\n",
    "            'speaker_type': speaker_id,\n",
    "            'speaker_text': speaker,\n",
    "            'relation_type': relation_type_id,\n",
    "            'relation_text': relation_type\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des relations\n",
    "def create_relation_dict(discourse_graph):\n",
    "    relation_set = set()\n",
    "\n",
    "    # Collecter toutes les relations uniques\n",
    "    for edge in discourse_graph:\n",
    "        relation_set.add(edge[1])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    relation_dict = {relation: idx for idx, relation in enumerate(relation_set)}\n",
    "\n",
    "    return relation_dict\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des speakers\n",
    "def create_speaker_dict(transcription):\n",
    "    speaker_set = set()\n",
    "\n",
    "    # Collecter tous les locuteurs uniques\n",
    "    for utterance in transcription:\n",
    "        speaker_set.add(utterance['speaker'])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    speaker_dict = {speaker: idx for idx, speaker in enumerate(speaker_set)}\n",
    "\n",
    "    return speaker_dict\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "# Function to get labels for a dialogue\n",
    "def get_label(dialogue_id, index,labels_data):\n",
    "    return labels_data.get(dialogue_id, [])[index]\n",
    "\n",
    "# Définition de la fonction F1-score\n",
    "def f1_score_keras(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return K.mean(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train= Path(\"data/training\")\n",
    "path_test= Path(\"data/test\")\n",
    "\n",
    "dialogue_ids = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "dialogue_ids = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids])\n",
    "dialogue_ids.remove('IS1002a')\n",
    "dialogue_ids.remove('IS1005d')\n",
    "dialogue_ids.remove('TS3012c')\n",
    "\n",
    "dialogue_ids_test = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "dialogue_ids_test = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids_test])\n",
    "\n",
    "# Liste pour stocker les DataFrames de chaque dialogue\n",
    "dfs = []\n",
    "dfs_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_train / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_train / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs.append(df)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': 0,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': 0,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs.append(pd.DataFrame([last_row]))\n",
    "\n",
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_test / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_test / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df_test = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs_test.append(df_test)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': 0,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': 0,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs_test.append(pd.DataFrame([last_row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer tous les DataFrames en un seul\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "with open(\"data/training_labels.json\", 'r') as file:\n",
    "    labels_data = json.load(file)\n",
    "\n",
    "df['label'] = df.apply(lambda row: get_label(row['dialogue_id'], row['index_start'], labels_data), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcription_id in dialogue_ids:\n",
    "    bert_array = np.load('training/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df[df['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array):\n",
    "        df.at[indices[idx-1], 'text'] = value\n",
    "\n",
    "for transcription_id in dialogue_ids_test:\n",
    "    bert_array_test = np.load('test/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df_test[df_test['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array_test):\n",
    "        df_test.at[indices[idx-1], 'text'] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'éléments dans chaque liste\n",
    "num_elements = len(df['text'].iloc[0])\n",
    "\n",
    "# Créez de nouvelles colonnes pour chaque élément dans la liste\n",
    "new_columns = [f'coord_{i}' for i in range(num_elements)]\n",
    "\n",
    "# Appliquez une fonction qui divise chaque liste en plusieurs colonnes\n",
    "new_text_columns = df['text'].apply(pd.Series)\n",
    "\n",
    "# Renommez les nouvelles colonnes avec les noms spécifiques\n",
    "new_text_columns.columns = new_columns\n",
    "\n",
    "# Concaténez les nouvelles colonnes avec le DataFrame existant\n",
    "df = pd.concat([df, new_text_columns], axis=1)\n",
    "\n",
    "# Supprimez la colonne 'text' originale si nécessaire\n",
    "df = df.drop(['text','speaker_text','relation_text' ], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nombre d'éléments dans chaque liste\n",
    "num_elements = len(df_test['text'].iloc[0])\n",
    "\n",
    "# Appliquez une fonction qui divise chaque liste en plusieurs colonnes\n",
    "new_text_columns_test = df_test['text'].apply(pd.Series)\n",
    "\n",
    "# Renommez les nouvelles colonnes avec les noms spécifiques\n",
    "new_text_columns_test.columns = new_columns\n",
    "\n",
    "# Concaténez les nouvelles colonnes avec le DataFrame existant\n",
    "df_test = pd.concat([df_test, new_text_columns_test], axis=1)\n",
    "\n",
    "# Supprimez la colonne 'text' originale si nécessaire\n",
    "df_test = df_test.drop(['text','speaker_text','relation_text' ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_index'] = df['index_end'] - df['index_start']\n",
    "df_test['diff_index'] = df_test['index_end'] - df_test['index_start']\n",
    "\n",
    "df_init = df.copy()\n",
    "df_init_test = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour encoder dialogue_id\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['dialogue_id'] = label_encoder.fit_transform(df['dialogue_id'])\n",
    "df_test['dialogue_id'] = label_encoder.fit_transform(df_test['dialogue_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes pour la matrice de corrélation\n",
    "selected_columns = ['index_start', 'index_end', 'speaker_type', 'relation_type', 'label', 'diff_index']\n",
    "\n",
    "# Créer un sous-dataframe avec les colonnes sélectionnées\n",
    "corr_df = df[selected_columns]\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "# Tracer la matrice de corrélation colorée\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "#plt.title('Matrice de Corrélation')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X = df.drop(['label'], axis=1)\n",
    "\n",
    "# Sélectionner la colonne 'label' comme y_train\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "y_train_ohe = to_categorical(y_train, num_classes=2)\n",
    "y_test_ohe = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "y_ohe = to_categorical(y, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001 #best so far 0.001\n",
    "nb_epochs = 3\n",
    "batch_nb = 32\n",
    "val_split = 0.3\n",
    "taux_drop = 0.3\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Ajout callback checkpoint\n",
    "checkpoint = ModelCheckpoint('best_model.h5',\n",
    "                             monitor='val_f1_score_keras',\n",
    "                             save_best_only=True,\n",
    "                             mode='max',\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1113/1113 [==============================] - 4s 3ms/step - loss: 10.4877 - auc_6: 0.5518 - f1_score_keras: 0.4682 - val_loss: 0.7744 - val_auc_6: 0.7410 - val_f1_score_keras: 0.4737\n",
      "Epoch 2/3\n",
      "1113/1113 [==============================] - 3s 2ms/step - loss: 3.4923 - auc_6: 0.5186 - f1_score_keras: 0.4636 - val_loss: 0.6681 - val_auc_6: 0.6549 - val_f1_score_keras: 0.5577\n",
      "Epoch 3/3\n",
      "1113/1113 [==============================] - 3s 2ms/step - loss: 1.0478 - auc_6: 0.5193 - f1_score_keras: 0.4712 - val_loss: 0.6947 - val_auc_6: 0.5440 - val_f1_score_keras: 0.4938\n",
      "681/681 [==============================] - 1s 901us/step\n"
     ]
    }
   ],
   "source": [
    "#Réseaux de neurones\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, Input, LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(390))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(taux_drop))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Utilisation de la fonction F1-score comme métrique\n",
    "model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=[AUC(), f1_score_keras])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "model.fit(X_train, y_train_ohe, epochs=nb_epochs, batch_size=batch_nb, validation_split=val_split, class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 896/1589 [===============>..............] - ETA: 1s - loss: 8.0338 - auc_7: 0.4846 - f1_score_keras: 0.4440"
     ]
    }
   ],
   "source": [
    "#Réseaux de neurones\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, Input, LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Input(390))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(taux_drop))\n",
    "model2.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=[AUC(), f1_score_keras])\n",
    "\n",
    "model2.fit(X, y_ohe, epochs=nb_epochs, batch_size=batch_nb, validation_split=val_split, class_weight=dict(enumerate(class_weights)), callbacks=[checkpoint])\n",
    "test_pred = model2.predict(df_test)\n",
    "test_labels = np.argmax(test_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb 1 pour la validation: 4490\n",
      "Nb theorique validation: 17689\n",
      "F1-score validation: 0.2891319905940088 \n",
      "\n",
      "Nb 1 sur test: 30632\n",
      "Dans l'idée, environ 7500\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb 1 pour la validation:\", sum(y_pred_class))\n",
    "print(\"Nb theorique validation:\", sum(y))\n",
    "print(\"F1-score validation:\", f1_score(y_test, y_pred_class),'\\n')\n",
    "\n",
    "print(\"Nb 1 sur test:\", sum(test_labels))\n",
    "print(\"Dans l'idée, environ 7500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire pour stocker les prédictions\n",
    "test_labels_final = {dialogue_id: [] for dialogue_id in dialogue_ids_test}\n",
    "\n",
    "# Parcourir les lignes de df_test\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Obtenez les indices des lignes correspondant au dialogue_id\n",
    "    indices = df_init_test[df_init_test['dialogue_id'] == dialogue_id].index\n",
    "    # Ajouter les valeurs de test_label[index] au dictionnaire\n",
    "    test_labels_final[dialogue_id] = test_labels[indices].tolist()\n",
    "\n",
    "\n",
    "with open(\"test_labels_text_mlp.json\", \"w\") as file:\n",
    "    json.dump(test_labels_final, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
