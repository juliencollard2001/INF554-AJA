{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicegorge/INF554/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training = Path(\"data/training\")\n",
    "path_to_test = Path(\"data/test\")\n",
    "\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training():\n",
    "\n",
    "    N_files = len(training_set)\n",
    "    D_embedding = 384\n",
    "\n",
    "    graphs = [] \n",
    "\n",
    "    # lets got throug all training graphs\n",
    "    for k, transcription_id in enumerate(training_set):\n",
    "        #nodes\n",
    "        bert_array = np.load('training/' + transcription_id +'.npy')\n",
    "        x = torch.tensor(bert_array.reshape(-1,384), dtype=torch.float)\n",
    "        #edges\n",
    "        edges = [[] for _ in range(16)]\n",
    "        with open(path_to_training / f\"{transcription_id}.txt\", \"r\") as graphe:\n",
    "            for line in graphe:\n",
    "                l = line.split()\n",
    "                i = int(l[0])\n",
    "                j =  int(l[2])\n",
    "                edge_type = label2int[l[1]] - 1\n",
    "                edges[edge_type].append([i,j])\n",
    "        edges = [torch.tensor(edges[k]).t().contiguous() for k in range(16)]\n",
    "        #labels\n",
    "        with open(\"data/training_labels.json\", \"r\") as file:\n",
    "            training_labels = json.load(file)\n",
    "        labels = torch.tensor(np.array(training_labels[transcription_id]))\n",
    "        graph = Data(x=x, edge_index=edges, y=labels)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "def extract_test():\n",
    "\n",
    "    N_files = len(training_set)\n",
    "    D_embedding = 384\n",
    "\n",
    "    graphs = [] \n",
    "\n",
    "    # lets got throug all training graphs\n",
    "    for k, transcription_id in enumerate(test_set):\n",
    "        #nodes\n",
    "        bert_array = np.load('test/' + transcription_id +'.npy')\n",
    "        x = torch.tensor(bert_array.reshape(-1,384), dtype=torch.float)\n",
    "        #edges\n",
    "        edges = [[] for _ in range(16)]\n",
    "        with open(path_to_test / f\"{transcription_id}.txt\", \"r\") as graphe:\n",
    "            for line in graphe:\n",
    "                l = line.split()\n",
    "                i = int(l[0])\n",
    "                j =  int(l[2])\n",
    "                edge_type = label2int[l[1]] - 1\n",
    "                edges[edge_type].append([i,j])\n",
    "        edges = [torch.tensor(edges[k]).t().contiguous() for k in range(16)]\n",
    "        \n",
    "        graph = Data(x=x, edge_index=edges)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "def f1_score(y_pred, y_real):\n",
    "    conf_matrix = confusion_matrix(y_real, y_pred)\n",
    "    tp, fp, fn, tn = conf_matrix[1, 1], conf_matrix[0, 1], conf_matrix[1, 0], conf_matrix[0, 0]\n",
    "    if (tp + fp) == 0:\n",
    "        return 0\n",
    "    if (tp + fn) == 0:\n",
    "        return 0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "graph_links_labels= set()\n",
    "for id in training_set:\n",
    "    with open(path_to_training / f\"{id}.txt\", \"r\") as graphe:\n",
    "        for line in graphe:\n",
    "            l = line.split()\n",
    "            graph_links_labels.add(l[1])\n",
    "L = list(graph_links_labels)\n",
    "\n",
    "int2label = {indice: valeur for indice, valeur in enumerate(L)}\n",
    "label2int = {valeur: indice for indice, valeur in enumerate(L)}\n",
    "\n",
    "N_vocab_links = len(L)\n",
    "print(N_vocab_links)\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "data = extract_training()\n",
    "data_test_kagle = extract_test()\n",
    "\n",
    "data_train = data[:-nb_test]\n",
    "data_test = data[-nb_test:]\n",
    "train_loader = DataLoader(data_train)\n",
    "test_loader = DataLoader(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.sc1 = 100\n",
    "        self.f1 = 50\n",
    "        self.sc2 = 30\n",
    "        self.GCN1 = nn.ModuleList([GCNConv(input_dim, self.sc1) for _ in range(channels)])\n",
    "        self.dense1 = nn.Linear(self.sc1*channels, self.f1)\n",
    "        self.GCN2 = nn.ModuleList([GCNConv(self.f1, self.sc2) for _ in range(channels)])\n",
    "        self.dense2 = nn.Linear(self.sc2*channels, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "        # Appliquez les couches GCN avec une activation ReLU entre elles\n",
    "        x1 = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.sc1)\n",
    "            else:\n",
    "                x = F.relu(self.GCN1[k](nodes, edges[k]))\n",
    "            x1.append(x)\n",
    "        x1_f = torch.cat(x1, dim=1)\n",
    "\n",
    "        f1 = F.relu(self.dense1(x1_f))\n",
    "\n",
    "        x2 = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.sc2)\n",
    "            else:\n",
    "                x = F.relu(self.GCN2[k](f1, edges[k]))\n",
    "            x1.append(x)\n",
    "            x2.append(x)\n",
    "        x2_f = torch.cat(x2, dim=1)\n",
    "\n",
    "        x_out = self.dense2(x2_f)\n",
    "\n",
    "        return F.log_softmax(x_out, dim=1)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(batch)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        y_true = batch.y.cpu().numpy()\n",
    "        f1 = f1_score(y_true, predicted.cpu().numpy())\n",
    "        return {'val_f1': torch.tensor(f1)}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "        return {'val_f1': avg_f1.item()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Data\n",
      "13292 labels 1 sur  72623 noeuds soit 18.30274155570549 %\n",
      "-----------------------------\n",
      "Training set : 77 graphs\n",
      "10580 labels 1 sur  53802 noeuds soit 19.66469647968477 %\n",
      "Testing set : 20\n",
      "2712 labels 1 sur  18821 noeuds soit 14.409436267998512 %\n",
      "-----------------------------\n",
      "Epoch: 000, Loss: 20.8156\n",
      "F1-score: 0.0\n",
      "Epoch: 001, Loss: 18.2129\n",
      "F1-score: 0.2795075375251485\n",
      "Epoch: 002, Loss: 17.7556\n",
      "F1-score: 0.3747611842656492\n",
      "Epoch: 003, Loss: 17.2653\n",
      "F1-score: 0.20989981068071634\n",
      "Epoch: 004, Loss: 16.6812\n",
      "F1-score: 0.45048042372877484\n",
      "Epoch: 005, Loss: 15.8838\n",
      "F1-score: 0.4312733837359194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class ajout df.ipynb Cellule 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-----------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m15\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     test_during_training()\n",
      "\u001b[1;32m/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class ajout df.ipynb Cellule 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss_tot \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     out \u001b[39m=\u001b[39m model(data)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mlong())  \u001b[39m# Compute the loss solely based on the training nodes.\u001b[39;00m\n",
      "File \u001b[0;32m~/INF554/miniconda3/lib/python3.11/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/INF554/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/INF554/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:808\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[39mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 808\u001b[0m         p\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mgrad_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instanciez le modèle\n",
    "model = NodeClassifier(16,384)\n",
    "\n",
    "# Définissez la fonction de perte et l'optimiseur\n",
    "f = 0.3\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1-f, f]))\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraînez le modèle\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_tot = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y.long())  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        loss_tot += loss\n",
    "    return loss_tot\n",
    "\n",
    "def test_during_training():\n",
    "    model.eval()\n",
    "    S = 0\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        #print(predicted.numpy())\n",
    "        f1 = f1_score(predicted.numpy(), data.y.numpy())\n",
    "        S += f1\n",
    "    f1_moyen = S / len(test_loader)\n",
    "    f1_naive = f1_score(np.ones(len(predicted), dtype=int), data.y.numpy())\n",
    "    print(f'F1-score: {f1_moyen}')\n",
    "\n",
    "def prediction(graph):\n",
    "    model.eval()\n",
    "    out = model(graph)\n",
    "    _, predicted = torch.max(out, 1)\n",
    "    return predicted.numpy()\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print('Data')\n",
    "ones = sum([np.sum(g.y.numpy()) for g in data])\n",
    "tot_nodes = sum([g.x.numpy().shape[0] for g in data])\n",
    "print(ones, 'labels 1 sur ',tot_nodes,'noeuds soit', 100*ones/tot_nodes, '%')\n",
    "print(\"-----------------------------\")\n",
    "print('Training set :', len(data_train),'graphs')\n",
    "ones = sum([np.sum(g.y.numpy()) for g in data_train])\n",
    "tot_nodes = sum([g.x.numpy().shape[0] for g in data_train])\n",
    "print(ones, 'labels 1 sur ',tot_nodes,'noeuds soit', 100*ones/tot_nodes, '%')\n",
    "print('Testing set :', len(data_test))\n",
    "ones_test = sum([np.sum(g.y.numpy()) for g in data_test])\n",
    "tot_nodes_test = sum([g.x.numpy().shape[0] for g in data_test])\n",
    "print(ones_test, 'labels 1 sur ',tot_nodes_test,'noeuds soit', 100*ones_test/tot_nodes_test, '%')\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "for epoch in range(6):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    test_during_training()\n",
    "\n",
    "ones_predicted = sum([np.sum(prediction(g)) for g in test_loader])\n",
    "print(\"-----------------------------\")\n",
    "print(\"Test du modèle :\")\n",
    "print(ones_predicted, 'label 1 prédits sur les ',ones_test,'voulus (',100 * ones_predicted/tot_nodes_test,'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "\n",
    "for i, graph in enumerate(data_test_kagle):\n",
    "    id = test_set[i]\n",
    "    y_test = prediction(graph)\n",
    "    test_labels[id] = y_test.tolist()\n",
    "\n",
    "with open(\"test_labels_GNN.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
