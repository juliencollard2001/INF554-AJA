{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "def read_transcription(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dialogue_id, transcription):\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through all sentences in the transcription\n",
    "    for index, sentence in enumerate(transcription):\n",
    "        speaker = sentence['speaker']\n",
    "\n",
    "        # Get the sentence text\n",
    "        text = sentence['text']\n",
    "\n",
    "        # Add a row to the DataFrame\n",
    "        rows.append({\n",
    "            'dialogue_id': dialogue_id,\n",
    "            'index': index,\n",
    "            'text': text,\n",
    "            'speaker_text': speaker,\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to get labels for a dialogue\n",
    "def get_label(dialogue_id, index,labels_data):\n",
    "    return labels_data.get(dialogue_id, [])[index]\n",
    "\n",
    "#Fonctions\n",
    "def compter_mots(phrase):\n",
    "    mots = phrase.split()  # Divisez la phrase en mots en utilisant les espaces comme délimiteurs\n",
    "    return len(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training = Path(\"data/training\")\n",
    "path_to_test = Path(\"data/test\")\n",
    "\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
    "\n",
    "# Créer le DataFrame pour l'ensemble d'entraînement\n",
    "dfs = []\n",
    "for dialogue_id in training_set:\n",
    "    transcription_data = read_transcription(path_to_training / f'{dialogue_id}.json')\n",
    "    df = create_dataframe(dialogue_id, transcription_data)\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Ajouter la colonne 'label' à df en utilisant la fonction get_label\n",
    "with open(\"data/training_labels.json\", 'r') as file:\n",
    "    labels_data = json.load(file)\n",
    "\n",
    "df['label'] = df.apply(lambda row: get_label(row['dialogue_id'], row['index'], labels_data), axis=1)\n",
    "\n",
    "# Créer le DataFrame pour l'ensemble de test\n",
    "dfs_test = []\n",
    "for dialogue_id in test_set:\n",
    "    transcription_data = read_transcription(path_to_test / f'{dialogue_id}.json')\n",
    "    df_test = create_dataframe(dialogue_id, transcription_data)\n",
    "    dfs_test.append(df_test)\n",
    "\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ajout features\n",
    "\n",
    "df['nb_mots'] = df['text'].apply(compter_mots)\n",
    "df['nb_interrogations'] = df['text'].apply(lambda x: x.count('?'))\n",
    "df['interjections'] = df['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df['nb_words_more_5'] = df['text'].apply(lambda x: sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df = pd.concat([df, pd.get_dummies(df['speaker_text'], prefix='speaker_text', dtype=int)], axis=1) \n",
    "\n",
    "\n",
    "df_test['nb_mots'] = df_test['text'].apply(compter_mots)\n",
    "df_test['nb_interrogations'] = df_test['text'].apply(lambda x: x.count('?'))\n",
    "df_test['interjections'] = df_test['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_test['nb_words_more_5'] = df_test['text'].apply(lambda x: sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_test = pd.concat([df_test, pd.get_dummies(df_test['speaker_text'], prefix='speaker_text', dtype=int)], axis=1) \n",
    "\n",
    "import difflib\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_similarity(str1, str2):\n",
    "    seq = difflib.SequenceMatcher(None, str1, str2)\n",
    "    return seq.ratio()\n",
    "\n",
    "def calculate_max_similarity(graph, node):\n",
    "    neighbors = list(graph.neighbors(node))\n",
    "    if not neighbors:\n",
    "        return 0.0  # Si le nœud n'a pas de voisins, la similarité est zéro\n",
    "    similarities = [calculate_similarity(graph.nodes[node]['text'], graph.nodes[neighbor]['text']) for neighbor in neighbors]\n",
    "    return max(similarities)\n",
    "\n",
    "for node in graph.nodes:\n",
    "    graph.nodes[node]['similarities'] = calculate_max_similarity(graph, node)\n",
    "    \n",
    "df.head()\n",
    "df_test.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training():\n",
    "\n",
    "    N_files = len(training_set)\n",
    "    D_embedding = 384\n",
    "\n",
    "    graphs = [] \n",
    "\n",
    "    # lets got throug all training graphs\n",
    "    for k, transcription_id in enumerate(training_set):\n",
    "        #nodes\n",
    "        bert_array = np.load('training/' + transcription_id +'.npy')\n",
    "        x = torch.tensor(bert_array.reshape(-1,384), dtype=torch.float)\n",
    "        #edges\n",
    "        edges = [[] for _ in range(16)]\n",
    "        with open(path_to_training / f\"{transcription_id}.txt\", \"r\") as graphe:\n",
    "            for line in graphe:\n",
    "                l = line.split()\n",
    "                i = int(l[0])\n",
    "                j =  int(l[2])\n",
    "                edge_type = label2int[l[1]] - 1\n",
    "                edges[edge_type].append([i,j])\n",
    "        edges = [torch.tensor(edges[k]).t().contiguous() for k in range(16)]\n",
    "        #labels\n",
    "        with open(\"data/training_labels.json\", \"r\") as file:\n",
    "            training_labels = json.load(file)\n",
    "        labels = torch.tensor(np.array(training_labels[transcription_id]))\n",
    "        graph = Data(x=x, edge_index=edges, y=labels)\n",
    "        graphs.append(graph)\n",
    "    return graphs \n",
    "\n",
    "\n",
    "def extract_test():\n",
    "\n",
    "    N_files = len(training_set)\n",
    "    D_embedding = 384\n",
    "\n",
    "    graphs = [] \n",
    "\n",
    "    # lets got throug all training graphs\n",
    "    for k, transcription_id in enumerate(test_set):\n",
    "        #nodes\n",
    "        bert_array = np.load('test/' + transcription_id +'.npy')\n",
    "        x = torch.tensor(bert_array.reshape(-1,384), dtype=torch.float)\n",
    "        #edges\n",
    "        edges = [[] for _ in range(16)]\n",
    "        with open(path_to_test / f\"{transcription_id}.txt\", \"r\") as graphe:\n",
    "            for line in graphe:\n",
    "                l = line.split()\n",
    "                i = int(l[0])\n",
    "                j =  int(l[2])\n",
    "                edge_type = label2int[l[1]] - 1\n",
    "                edges[edge_type].append([i,j])\n",
    "        edges = [torch.tensor(edges[k]).t().contiguous() for k in range(16)]\n",
    "        \n",
    "        graph = Data(x=x, edge_index=edges)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "def f1_score(y_pred, y_real):\n",
    "    conf_matrix = confusion_matrix(y_real, y_pred)\n",
    "    tp, fp, fn, tn = conf_matrix[1, 1], conf_matrix[0, 1], conf_matrix[1, 0], conf_matrix[0, 0]\n",
    "    if (tp + fp) == 0:\n",
    "        return 0\n",
    "    if (tp + fn) == 0:\n",
    "        return 0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "graph_links_labels= set()\n",
    "for id in training_set:\n",
    "    with open(path_to_training / f\"{id}.txt\", \"r\") as graphe:\n",
    "        for line in graphe:\n",
    "            l = line.split()\n",
    "            graph_links_labels.add(l[1])\n",
    "L = list(graph_links_labels)\n",
    "\n",
    "print(type(graph_links_labels))\n",
    "\n",
    "int2label = {indice: valeur for indice, valeur in enumerate(L)}\n",
    "label2int = {valeur: indice for indice, valeur in enumerate(L)}\n",
    "\n",
    "N_vocab_links = len(L)\n",
    "print(N_vocab_links)\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "data = extract_training()\n",
    "data_test_kagle = extract_test()\n",
    "\n",
    "data_train = data[:-nb_test]\n",
    "data_test = data[-nb_test:]\n",
    "train_loader = DataLoader(data_train)\n",
    "test_loader = DataLoader(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class ajout df.ipynb Cellule 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m data:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(graph\u001b[39m.\u001b[39mnum_nodes):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         graph\u001b[39m.\u001b[39mx[node] \u001b[39m=\u001b[39m compter_mots(graph\u001b[39m.\u001b[39;49mx[node][\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Ajout de la colonne 'nb_interrogations' aux nœuds de chaque graphe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alicegorge/Desktop/X/INF554/AJA/INF554-AJA/Alice/node_class%20ajout%20df.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m data:\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_similarity(str1, str2):\n",
    "    seq = difflib.SequenceMatcher(None, str1, str2)\n",
    "    return seq.ratio()\n",
    "\n",
    "def calculate_max_similarity(graph, node):\n",
    "    neighbors = list(graph.neighbors(node))\n",
    "    if not neighbors:\n",
    "        return 0.0  # Si le nœud n'a pas de voisins, la similarité est zéro\n",
    "    similarities = [calculate_similarity(graph.nodes[node]['text'], graph.nodes[neighbor]['text']) for neighbor in neighbors]\n",
    "    return max(similarities)\n",
    "\n",
    "\n",
    "# Ajout de la colonne 'nb_mots' aux nœuds de chaque graphe\n",
    "for graph in data:\n",
    "    for node in range(graph.num_nodes):\n",
    "        graph.x[node] = compter_mots(graph.x[node]['text'])\n",
    "\n",
    "# Ajout de la colonne 'nb_interrogations' aux nœuds de chaque graphe\n",
    "for graph in data:\n",
    "    for node in range(graph.num_nodes):\n",
    "        graph.x[node] = graph.x[node]['text'].count('?')\n",
    "\n",
    "# Ajout de la colonne 'interjections' aux nœuds de chaque graphe\n",
    "for graph in data:\n",
    "    for node in range(graph.num_nodes):\n",
    "        graph.x[node] = sum(graph.x[node]['text'].split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh'])\n",
    "\n",
    "# Ajout de la colonne 'nb_words_more_5' aux nœuds de chaque graphe\n",
    "for graph in data:\n",
    "    for node in range(graph.num_nodes):\n",
    "        graph.x[node] = sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in graph.x[node]['text'].split())\n",
    "\n",
    "# Ajout de la colonne 'similarities' aux nœuds de chaque graphe\n",
    "for graph in data:\n",
    "    for node in range(graph.num_nodes):\n",
    "        graph.x[node] = calculate_max_similarity(graph, node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.sc1 = 100\n",
    "        self.f1 = 50\n",
    "        self.sc2 = 30\n",
    "        self.GCN1 = nn.ModuleList([GCNConv(input_dim, self.sc1) for _ in range(channels)])\n",
    "        self.dense1 = nn.Linear(self.sc1*channels, self.f1)\n",
    "        self.GCN2 = nn.ModuleList([GCNConv(self.f1, self.sc2) for _ in range(channels)])\n",
    "        self.dense2 = nn.Linear(self.sc2*channels, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "        # Appliquez les couches GCN avec une activation ReLU entre elles\n",
    "        x1 = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.sc1)\n",
    "            else:\n",
    "                x = F.relu(self.GCN1[k](nodes, edges[k]))\n",
    "            x1.append(x)\n",
    "        x1_f = torch.cat(x1, dim=1)\n",
    "\n",
    "        f1 = F.relu(self.dense1(x1_f))\n",
    "\n",
    "        x2 = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.sc2)\n",
    "            else:\n",
    "                x = F.relu(self.GCN2[k](f1, edges[k]))\n",
    "            x1.append(x)\n",
    "            x2.append(x)\n",
    "        x2_f = torch.cat(x2, dim=1)\n",
    "\n",
    "        x_out = self.dense2(x2_f)\n",
    "\n",
    "        return F.log_softmax(x_out, dim=1)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self(batch)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        y_true = batch.y.cpu().numpy()\n",
    "        f1 = f1_score(y_true, predicted.cpu().numpy())\n",
    "        return {'val_f1': torch.tensor(f1)}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "        return {'val_f1': avg_f1.item()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Data\n",
      "13292 labels 1 sur  72623 noeuds soit 18.30274155570549 %\n",
      "-----------------------------\n",
      "Training set : 77 graphs\n",
      "10580 labels 1 sur  53802 noeuds soit 19.66469647968477 %\n",
      "Testing set : 20\n",
      "2712 labels 1 sur  18821 noeuds soit 14.409436267998512 %\n",
      "-----------------------------\n",
      "Epoch: 000, Loss: 22.1087\n",
      "F1-score: 0.0\n",
      "Epoch: 001, Loss: 18.3699\n",
      "F1-score: 0.0\n",
      "Epoch: 002, Loss: 17.8221\n",
      "F1-score: 0.0484051866588115\n",
      "Epoch: 003, Loss: 17.3807\n",
      "F1-score: 0.35568526581829174\n",
      "Epoch: 004, Loss: 16.8502\n",
      "F1-score: 0.4219553010187026\n",
      "Epoch: 005, Loss: 16.1520\n",
      "F1-score: 0.45152881623651037\n",
      "Epoch: 006, Loss: 15.1563\n",
      "F1-score: 0.45625315708983943\n",
      "Epoch: 007, Loss: 13.6186\n",
      "F1-score: 0.42132560242927\n",
      "Epoch: 008, Loss: 12.0331\n",
      "F1-score: 0.4343665452484182\n",
      "Epoch: 009, Loss: 9.8330\n",
      "F1-score: 0.46546113743796\n",
      "Epoch: 010, Loss: 7.9404\n",
      "F1-score: 0.4564676590673879\n",
      "Epoch: 011, Loss: 6.9806\n",
      "F1-score: 0.44574313172260166\n",
      "Epoch: 012, Loss: 7.0860\n",
      "F1-score: 0.46233336047443024\n",
      "Epoch: 013, Loss: 8.9570\n",
      "F1-score: 0.46215210276610774\n",
      "Epoch: 014, Loss: 9.7716\n",
      "F1-score: 0.45890042640138173\n",
      "Epoch: 015, Loss: 7.7679\n",
      "F1-score: 0.4240873727625232\n",
      "Epoch: 016, Loss: 5.3602\n",
      "F1-score: 0.4434385646123912\n",
      "Epoch: 017, Loss: 3.2544\n",
      "F1-score: 0.4443095432826228\n",
      "Epoch: 018, Loss: 2.4965\n",
      "F1-score: 0.4385546486739244\n",
      "Epoch: 019, Loss: 2.1045\n",
      "F1-score: 0.43423157430049814\n",
      "Epoch: 020, Loss: 1.9261\n",
      "F1-score: 0.4489309374152565\n",
      "Epoch: 021, Loss: 2.0206\n",
      "F1-score: 0.4383155473533059\n",
      "Epoch: 022, Loss: 2.5173\n",
      "F1-score: 0.4238008617932369\n",
      "Epoch: 023, Loss: 3.0048\n",
      "F1-score: 0.40254870887243965\n",
      "Epoch: 024, Loss: 3.1670\n",
      "F1-score: 0.36762676077580364\n",
      "-----------------------------\n",
      "Test du modèle :\n",
      "2292 label 1 prédits sur les  2712 voulus ( 12.177886403485468 %)\n"
     ]
    }
   ],
   "source": [
    "# Instanciez le modèle\n",
    "model = NodeClassifier(16,384)\n",
    "\n",
    "# Définissez la fonction de perte et l'optimiseur\n",
    "f = 0.3\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1-f, f]))\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînez le modèle\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_tot = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y.long())  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        loss_tot += loss\n",
    "    return loss_tot\n",
    "\n",
    "def test_during_training():\n",
    "    model.eval()\n",
    "    S = 0\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        #print(predicted.numpy())\n",
    "        f1 = f1_score(predicted.numpy(), data.y.numpy())\n",
    "        S += f1\n",
    "    f1_moyen = S / len(test_loader)\n",
    "    f1_naive = f1_score(np.ones(len(predicted), dtype=int), data.y.numpy())\n",
    "    print(f'F1-score: {f1_moyen}')\n",
    "\n",
    "def prediction(graph):\n",
    "    model.eval()\n",
    "    out = model(graph)\n",
    "    _, predicted = torch.max(out, 1)\n",
    "    return predicted.numpy()\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print('Data')\n",
    "ones = sum([np.sum(g.y.numpy()) for g in data])\n",
    "tot_nodes = sum([g.x.numpy().shape[0] for g in data])\n",
    "print(ones, 'labels 1 sur ',tot_nodes,'noeuds soit', 100*ones/tot_nodes, '%')\n",
    "print(\"-----------------------------\")\n",
    "print('Training set :', len(data_train),'graphs')\n",
    "ones = sum([np.sum(g.y.numpy()) for g in data_train])\n",
    "tot_nodes = sum([g.x.numpy().shape[0] for g in data_train])\n",
    "print(ones, 'labels 1 sur ',tot_nodes,'noeuds soit', 100*ones/tot_nodes, '%')\n",
    "print('Testing set :', len(data_test))\n",
    "ones_test = sum([np.sum(g.y.numpy()) for g in data_test])\n",
    "tot_nodes_test = sum([g.x.numpy().shape[0] for g in data_test])\n",
    "print(ones_test, 'labels 1 sur ',tot_nodes_test,'noeuds soit', 100*ones_test/tot_nodes_test, '%')\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "for epoch in range(25):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    test_during_training()\n",
    "\n",
    "ones_predicted = sum([np.sum(prediction(g)) for g in test_loader])\n",
    "print(\"-----------------------------\")\n",
    "print(\"Test du modèle :\")\n",
    "print(ones_predicted, 'label 1 prédits sur les ',ones_test,'voulus (',100 * ones_predicted/tot_nodes_test,'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "\n",
    "for i, graph in enumerate(data_test_kagle):\n",
    "    id = test_set[i]\n",
    "    y_test = prediction(graph)\n",
    "    test_labels[id] = y_test.tolist()\n",
    "\n",
    "with open(\"test_labels_GNN.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
