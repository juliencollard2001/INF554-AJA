{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../AJA')\n",
    "import AJA as aja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aja' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arnau\\OneDrive\\Documents\\Scolarité X\\3A\\INF554-AJA\\INF554-AJA\\arnaud\\bagging_arnaud.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# récupération des données \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_train_nodes, df_train_edges, df_test_nodes, df_test_edges \u001b[39m=\u001b[39m aja\u001b[39m.\u001b[39mget_data()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aja' is not defined"
     ]
    }
   ],
   "source": [
    "# récupération des données \n",
    "df_train_nodes, df_train_edges, df_test_nodes, df_test_edges = aja.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arnau\\OneDrive\\Documents\\Scolarité X\\3A\\INF554-AJA\\INF554-AJA\\arnaud\\bagging_arnaud.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# sentence length normalized\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df_train_nodes[\u001b[39m'\u001b[39m\u001b[39msentence_length\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_train_nodes[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m s: \u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39msplit()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df_train_nodes[\u001b[39m'\u001b[39m\u001b[39msentence_length\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(df_train_nodes[\u001b[39m'\u001b[39m\u001b[39msentence_length\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df_test_nodes[\u001b[39m'\u001b[39m\u001b[39msentence_length\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_test_nodes[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m s: \u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39msplit()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# node\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# sentence length normalized\n",
    "df_train_nodes['sentence_length'] = df_train_nodes['text'].apply(lambda s: len(s.split()))\n",
    "df_train_nodes['sentence_length'] = scaler.fit_transform(df_train_nodes['sentence_length'].values.reshape(-1, 1))\n",
    "df_test_nodes['sentence_length'] = df_test_nodes['text'].apply(lambda s: len(s.split()))\n",
    "df_test_nodes['sentence_length'] = scaler.transform(df_test_nodes['sentence_length'].values.reshape(-1, 1))\n",
    "\n",
    "df_train_nodes['nb_occurences'] = df_train_nodes['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_train_nodes['nb_occurences'] = scaler.fit_transform(df_train_nodes['nb_occurences'].values.reshape(-1, 1))\n",
    "df_test_nodes['nb_occurences'] = df_test_nodes['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_test_nodes['nb_occurences'] = scaler.transform(df_test_nodes['nb_occurences'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "df_train_nodes['nb_words_more_5'] = df_train_nodes['text'].apply(lambda x: sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_train_nodes['nb_words_more_5'] = scaler.fit_transform(df_train_nodes['nb_words_more_5'].values.reshape(-1, 1))\n",
    "df_test_nodes['nb_words_more_5'] = df_test_nodes['text'].apply(lambda x: sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_test_nodes['nb_words_more_5'] = scaler.transform(df_test_nodes['nb_words_more_5'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# speaker hot-one encoding\n",
    "one_hot_encoded = pd.get_dummies(df_train_nodes['speaker_int'], prefix='speaker', dtype=int)\n",
    "df_train_nodes = df_train_nodes.drop('speaker_int', axis=1)\n",
    "df_train_nodes = df_train_nodes.drop('speaker_text', axis=1)\n",
    "df_train_nodes = pd.concat([df_train_nodes, one_hot_encoded], axis=1)\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(df_test_nodes['speaker_int'], prefix='speaker', dtype=int)\n",
    "df_test_nodes = df_test_nodes.drop('speaker_int', axis=1)\n",
    "df_test_nodes = df_test_nodes.drop('speaker_text', axis=1)\n",
    "df_test_nodes = pd.concat([df_test_nodes, one_hot_encoded], axis=1)\n",
    "\n",
    "# TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_train_nodes['text'])\n",
    "df_train_nodes['tfidf_sum'] = tfidf_matrix.sum(axis=1)\n",
    "df_train_nodes['tfidf_max'] = tfidf_matrix.max(axis=1).toarray().flatten()\n",
    "\n",
    "tfidf_matrix_test = tfidf_vectorizer.fit_transform(df_test_nodes['text'])\n",
    "df_test_nodes['tfidf_sum'] = tfidf_matrix_test.sum(axis=1)\n",
    "df_test_nodes['tfidf_max'] = tfidf_matrix_test.max(axis=1).toarray().flatten()\n",
    "\n",
    "df_train_nodes['tfidf_sum'] = scaler.fit_transform(df_train_nodes['tfidf_sum'].values.reshape(-1,1))\n",
    "df_test_nodes['tfidf_sum'] = scaler.transform(df_test_nodes['tfidf_sum'].values.reshape(-1,1))\n",
    "\n",
    "df_train_nodes['tfidf_max'] = scaler.fit_transform(df_train_nodes['tfidf_max'].values.reshape(-1,1))\n",
    "df_test_nodes['tfidf_max'] = scaler.transform(df_test_nodes['tfidf_max'].values.reshape(-1,1))\n",
    "\n",
    "# Numbers\n",
    "df_train_nodes['has_number'] = df_train_nodes['text'].str.contains(r'\\d').astype(int)\n",
    "df_test_nodes['has_number'] = df_test_nodes['text'].str.contains(r'\\d').astype(int)\n",
    "\n",
    "# edge\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "        'transcription': df_train_edges['transcription'],\n",
    "        'start': df_train_edges['end'],\n",
    "        'end': df_train_edges['start'],\n",
    "        'type_int': 16 + df_train_edges['type_int'],\n",
    "        'type_text': df_train_edges['type_text'] + \"_reverse\"\n",
    "    })\n",
    "df_train_edges = pd.concat([df_train_edges, new_df], ignore_index=True)\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "        'transcription': df_test_edges['transcription'],\n",
    "        'start': df_test_edges['end'],\n",
    "        'end': df_test_edges['start'],\n",
    "        'type_int': 16 + df_test_edges['type_int'],\n",
    "        'type_text': df_test_edges['type_text'] + \"_reverse\"\n",
    "    })\n",
    "df_test_edges = pd.concat([df_test_edges, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des graphs\n",
    "train_graphs, test_graphs = aja.make_graphs(df_train_nodes, df_train_edges, df_test_nodes, df_test_edges)\n",
    "N_features = train_graphs['ES2002a'].x.shape[1]\n",
    "train_graphs, validation_graphs = aja.train_validation_split(train_graphs, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelsGCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, input_dim, post_conv_dim, output_dim, identity=False):\n",
    "        super(MultiChannelsGCN, self).__init__()\n",
    "        self.identity = identity\n",
    "        self.channels = channels\n",
    "        self.input_dim = input_dim\n",
    "        self.post_conv_dim = post_conv_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.GCN = nn.ModuleList([GCNConv(input_dim, post_conv_dim) for _ in range(channels)])\n",
    "        if identity:\n",
    "            self.dense = nn.Linear(post_conv_dim * (channels + 1), output_dim)\n",
    "            self.denseID = nn.Linear(input_dim, post_conv_dim)\n",
    "        else:\n",
    "            self.dense = nn.Linear(post_conv_dim * channels, output_dim)\n",
    "\n",
    "    def forward(self, nodes, edges):\n",
    "        X = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.post_conv_dim)\n",
    "            else:\n",
    "                x = F.relu(self.GCN[k](nodes, edges[k]))\n",
    "            X.append(x)\n",
    "        if self.identity:\n",
    "            X.append(F.relu(self.denseID(nodes)))\n",
    "        concat = torch.cat(X, dim=1)\n",
    "        return F.relu(self.dense(concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définie son plus beau modèle\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.GCN1 = MultiChannelsGCN(channels, input_dim, 50, 20, identity=True)\n",
    "        self.dense1 = nn.Linear(20,1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "        x = self.GCN1(nodes, edges)\n",
    "        x = self.dense1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, graph):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(graph)\n",
    "        return np.array((logits > 0.5).int()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class AjaPyTorchWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, train_graph_dict, validation_graph_dict, verbose=1, max_epochs=10):\n",
    "        # Training logic using your PyTorch model\n",
    "        # ...\n",
    "                \n",
    "        # Move the model and data to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "        # Use DataLoader to create batches of data\n",
    "        train_loader = DataLoader(list(train_graph_dict.values()), batch_size=1, shuffle=True)\n",
    "        N_train = len(train_loader)\n",
    "        validation_loader = DataLoader(list(validation_graph_dict.values()), batch_size=1, shuffle=False)\n",
    "        N_validation = len(validation_loader)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('Training on', N_train, 'graphs, validating on', N_validation, 'graphs')\n",
    "\n",
    "        # Train the model\n",
    "        model_name = \"model_py_torch\"\n",
    "        best_f1_score = 0\n",
    "        for epoch in range(max_epochs):\n",
    "            if verbose > 0:\n",
    "                print('- Epoch', f'{epoch + 1:03d}', '-')\n",
    "            # training\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data).squeeze()\n",
    "                loss = self.criterion(output, data.y.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            average_loss = total_loss / N_train\n",
    "            if verbose > 1:\n",
    "                print('Loss:', f'{average_loss:.4f}')\n",
    "            \n",
    "\n",
    "            # Evaluate the model on the training set\n",
    "            self.model.eval()\n",
    "            f1_moyen_train = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                y_pred = self.model.predict(data)\n",
    "                y_true = data.y.cpu().numpy()\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                f1_moyen_train += f1\n",
    "            f1_moyen_train /= N_train\n",
    "            if verbose > 1:\n",
    "                print('F1 train:', f1_moyen_train)\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            self.model.eval()\n",
    "            f1_moyen_valid = 0\n",
    "            for data in validation_loader:\n",
    "                data = data.to(device)\n",
    "                y_pred = self.model.predict(data)\n",
    "                y_true = data.y.cpu().numpy()\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                f1_moyen_valid += f1\n",
    "            f1_moyen_valid /= N_validation\n",
    "            if verbose > 1:\n",
    "                print('F1 valid:', f1_moyen_valid)\n",
    "\n",
    "            # callbacks ou autre\n",
    "            if f1_moyen_valid > best_f1_score:\n",
    "                if verbose > 1:\n",
    "                    print('It\\'s better !' )\n",
    "                torch.save(self.model.state_dict(), \"training_states/\" + model_name + \"-best.pth\")\n",
    "            else:\n",
    "                self.optimizer.param_groups[0]['lr'] /= 2\n",
    "                if verbose > 1:\n",
    "                    print('Learning rate reduced to:', self.optimizer.param_groups[0]['lr'])\n",
    "            if verbose > 1:\n",
    "                print('')\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print('Training finished !')\n",
    "\n",
    "        self.model.load_state_dict(torch.load(\"training_states/\" + model_name + \"-best.pth\"))\n",
    "\n",
    "    def predict(self, graphs_dict):\n",
    "        # Prediction logic using your PyTorch model\n",
    "        # ...\n",
    "        self.model.eval()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        result = {}\n",
    "        for key, graph in graphs_dict.items():\n",
    "            data = graph.to(device)\n",
    "            y_pred = self.model.predict(data)\n",
    "            result[key] = y_pred\n",
    "        return result\n",
    "\n",
    "\n",
    "    def score(self, graphs_dict):\n",
    "        # Scoring logic using your PyTorch model\n",
    "        # ...\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        validation_loader = DataLoader(list(graphs_dict.values()), batch_size=1, shuffle=False)\n",
    "        N_validation = len(validation_loader)\n",
    "        self.model.eval()\n",
    "        f1_moyen_valid = 0\n",
    "        for data in validation_loader:\n",
    "            data = data.to(device)\n",
    "            y_pred = self.model.predict(data)\n",
    "            y_true = data.y.cpu().numpy()\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            f1_moyen_valid += f1\n",
    "        f1_moyen_valid /= N_validation\n",
    "        return f1_moyen_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_moyen(pred_dict, true_graphs_dict):\n",
    "    f1_moyen = 0\n",
    "    for key, pred in pred_dict.items():\n",
    "        y_true = true_graphs_dict[key].y.numpy()\n",
    "        f1_moyen += f1_score(y_true, pred)\n",
    "    f1_moyen /= len(pred_dict)\n",
    "    return f1_moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 78 graphs, validating on 19 graphs\n",
      "- Epoch 001 -\n",
      "Loss: 0.9778\n",
      "F1 train: 0.5704299677573675\n",
      "F1 valid: 0.5814970897769062\n",
      "It's better !\n",
      "\n",
      "- Epoch 002 -\n",
      "Loss: 0.9660\n",
      "F1 train: 0.557919618940034\n",
      "F1 valid: 0.5639284182978541\n",
      "It's better !\n",
      "\n",
      "- Epoch 003 -\n",
      "Loss: 0.9632\n",
      "F1 train: 0.5800792358981276\n",
      "F1 valid: 0.5772523827480243\n",
      "It's better !\n",
      "\n",
      "Training finished !\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of your PyTorch model\n",
    "pytorch_model = NodeClassifier(32, N_features)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "alpha = 0.2\n",
    "gamma = 5\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((1 - alpha) / alpha), reduction='mean')\n",
    "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=0.01)\n",
    "\n",
    "# Create an instance of the custom wrapper\n",
    "model = AjaPyTorchWrapper(pytorch_model, criterion, optimizer)\n",
    "\n",
    "# Fit, predict, and score using scikit-learn-like API\n",
    "model.fit(train_graphs, validation_graphs, max_epochs=3,verbose=2)\n",
    "y_pred = model.predict(test_graphs)\n",
    "accuracy = model.score(validation_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_bagging_models(n_bagging, train_graphs):\n",
    "    models = []\n",
    "    for i in range(n_bagging):\n",
    "        print('Bagging', i+1)\n",
    "        pytorch_model = NodeClassifier(32, N_features)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((1 - alpha) / alpha), reduction='mean')\n",
    "        optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=0.01)\n",
    "        model = AjaPyTorchWrapper(pytorch_model, criterion, optimizer)\n",
    "        keys = list(train_graphs.keys())\n",
    "        bagging_train_graphs = {}\n",
    "        bagging_validation_graphs = {}\n",
    "        samples = random.choices(keys, k=len(keys))\n",
    "        c_train = 0\n",
    "        c_validation = 0\n",
    "        for key in keys:\n",
    "            if key in samples:\n",
    "                bagging_train_graphs[c_train] = train_graphs[key]\n",
    "                c_train += 1\n",
    "            else:\n",
    "                bagging_validation_graphs[c_validation] = train_graphs[key]\n",
    "                c_validation += 1\n",
    "        model.fit(bagging_train_graphs, bagging_validation_graphs, max_epochs=6, verbose=0)\n",
    "        models.append(model)\n",
    "        print('F1 score:', model.score(bagging_validation_graphs))\n",
    "    return models\n",
    "\n",
    "def predict_bagging(models, graphs_dict):\n",
    "    result = {}\n",
    "    for key, graph in graphs_dict.items():\n",
    "        y_pred = 0\n",
    "        for model in models:\n",
    "            y_pred += model.predict({key: graph})[key]\n",
    "        y_pred =  y_pred / len(models)\n",
    "        y_pred = np.array((y_pred > 0.5).astype(int)).flatten()\n",
    "        result[key] = y_pred\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging 1\n",
      "F1 score: 0.6038348779368814\n",
      "Bagging 2\n",
      "F1 score: 0.5556044393778441\n",
      "Bagging 3\n",
      "F1 score: 0.5916347610555189\n",
      "Bagging 4\n",
      "F1 score: 0.5924294554961383\n",
      "Bagging 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arnau\\OneDrive\\Documents\\Scolarité X\\3A\\INF554-AJA\\INF554-AJA\\arnaud\\bagging_arnaud.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m models \u001b[39m=\u001b[39m get_bagging_models(\u001b[39m10\u001b[39;49m, {\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_graphs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvalidation_graphs})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_prediction \u001b[39m=\u001b[39m predict_bagging(models, test_graphs)\n",
      "\u001b[1;32mc:\\Users\\arnau\\OneDrive\\Documents\\Scolarité X\\3A\\INF554-AJA\\INF554-AJA\\arnaud\\bagging_arnaud.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         bagging_validation_graphs[c_validation] \u001b[39m=\u001b[39m train_graphs[key]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         c_validation \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(bagging_train_graphs, bagging_validation_graphs, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m models\u001b[39m.\u001b[39mappend(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mF1 score:\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m.\u001b[39mscore(bagging_validation_graphs))\n",
      "\u001b[1;32mc:\\Users\\arnau\\OneDrive\\Documents\\Scolarité X\\3A\\INF554-AJA\\INF554-AJA\\arnaud\\bagging_arnaud.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(data)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/INF554-AJA/arnaud/bagging_arnaud.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = get_bagging_models(10, {**train_graphs, **validation_graphs})\n",
    "test_prediction = predict_bagging(models, test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "aja.make_test_csv_submission_from_dict(test_prediction, 'bagging_tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
