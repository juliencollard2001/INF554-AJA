{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Téléchargez les ressources nécessaires pour la lemmatisation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../AJA')\n",
    "import AJA as aja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données \n",
    "df_train_nodes, df_train_edges, df_test_nodes, df_test_edges = aja.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de prétraitement du texte\n",
    "def preprocess_text(text):\n",
    "    # Tokenisation\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Suppression de la ponctuation et des caractères spéciaux\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "    # Suppression des mots vides\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Reconstitution du texte à partir des tokens traités\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Appliquer le prétraitement aux colonnes 'text' dans les DataFrames\n",
    "df_train_nodes['text'] = df_train_nodes['text'].apply(preprocess_text)\n",
    "df_test_nodes['text'] = df_test_nodes['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer et le modèle BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fonction pour obtenir l'embedding BERT pour une phrase\n",
    "def get_bert_embedding(sentence):\n",
    "    # Tokeniser la phrase et l'encoder\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Obtenir l'embedding à partir du modèle BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Récupérer l'embedding de la couche d'output\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Appliquer la fonction d'embedding aux colonnes 'text' dans les DataFrames\n",
    "df_train_nodes['bert_embeddings'] = df_train_nodes['text'].apply(get_bert_embedding)\n",
    "df_test_nodes['bert_embeddings'] = df_test_nodes['text'].apply(get_bert_embedding)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
