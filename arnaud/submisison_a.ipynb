{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lire les données de transcription\n",
    "def read_transcription(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Fonction pour lire les données du graphe de discours\n",
    "def read_discourse_graph(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [line.strip().split() for line in file]\n",
    "\n",
    "    # Convertir les valeurs non numériques en indices numériques\n",
    "    data = [(int(start), relation, int(end)) if start.isdigit() and end.isdigit() else (start, relation, end) for start, relation, end in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict):\n",
    "    rows = []\n",
    "\n",
    "      # Iterate through all edges in the discourse graph\n",
    "    for edge in discourse_graph:\n",
    "        index_start, relation_type, index_end = edge\n",
    "\n",
    "        # Retrieve speaker information\n",
    "        speaker = transcription[index_start]['speaker']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        speaker_id = speaker_dict.get(speaker, -1)\n",
    "\n",
    "        # Retrieve the sentence\n",
    "        text = transcription[index_start]['text']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        relation_type_id = relation_dict.get(relation_type, -1)\n",
    "\n",
    "        # Add a row to the DataFrame\n",
    "        rows.append({\n",
    "            'dialogue_id': dialogue_id,\n",
    "            'index_start': index_start,\n",
    "            'text': text,\n",
    "            'index_end': index_end,\n",
    "            'speaker_type': speaker_id,\n",
    "            'speaker_text': speaker,\n",
    "            'relation_type': relation_type_id,\n",
    "            'relation_text': relation_type\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des relations\n",
    "def create_relation_dict(discourse_graph):\n",
    "    relation_set = set()\n",
    "\n",
    "    # Collecter toutes les relations uniques\n",
    "    for edge in discourse_graph:\n",
    "        relation_set.add(edge[1])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    relation_dict = {relation: idx for idx, relation in enumerate(relation_set)}\n",
    "\n",
    "    return relation_dict\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des speakers\n",
    "def create_speaker_dict(transcription):\n",
    "    speaker_set = set()\n",
    "\n",
    "    # Collecter tous les locuteurs uniques\n",
    "    for utterance in transcription:\n",
    "        speaker_set.add(utterance['speaker'])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    speaker_dict = {speaker: idx for idx, speaker in enumerate(speaker_set)}\n",
    "\n",
    "    return speaker_dict\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "# Function to get labels for a dialogue\n",
    "def get_label(dialogue_id, index,labels_data):\n",
    "    return labels_data.get(dialogue_id, [])[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacez 'votre_chemin' par le chemin correct\n",
    "path_train= Path(\"data/training\")\n",
    "path_test= Path(\"data/test\")\n",
    "\n",
    "# Remplacez 'vos_dialogue_ids' par votre liste réelle d'identifiants de dialogue\n",
    "dialogue_ids = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "dialogue_ids = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids])\n",
    "dialogue_ids.remove('IS1002a')\n",
    "dialogue_ids.remove('IS1005d')\n",
    "dialogue_ids.remove('TS3012c')\n",
    "\n",
    "dialogue_ids_test = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "dialogue_ids_test = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids_test])\n",
    "\n",
    "# Liste pour stocker les DataFrames de chaque dialogue\n",
    "dfs = []\n",
    "dfs_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_train / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_train / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs.append(df)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': np.nan,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': np.nan,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs.append(pd.DataFrame([last_row]))\n",
    "\n",
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_test / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_test / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df_test = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs_test.append(df_test)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': np.nan,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': np.nan,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs_test.append(pd.DataFrame([last_row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72623 entries, 0 to 72622\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   dialogue_id    72623 non-null  object \n",
      " 1   index_start    72623 non-null  int64  \n",
      " 2   text           72623 non-null  object \n",
      " 3   index_end      72526 non-null  float64\n",
      " 4   speaker_type   72623 non-null  int64  \n",
      " 5   speaker_text   72623 non-null  object \n",
      " 6   relation_type  72526 non-null  float64\n",
      " 7   relation_text  72526 non-null  object \n",
      " 8   label          72623 non-null  int64  \n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 5.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>index_start</th>\n",
       "      <th>text</th>\n",
       "      <th>index_end</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>relation_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>Right</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Explanation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>3</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>4</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id  index_start                                               text  \\\n",
       "0     ES2002a            0                                               Okay   \n",
       "1     ES2002a            1                                              Right   \n",
       "2     ES2002a            2  <vocalsound> Um well this is the kick-off meet...   \n",
       "3     ES2002a            3                             Um <vocalsound> and um   \n",
       "4     ES2002a            4  this is just what we're gonna be doing over th...   \n",
       "\n",
       "   index_end  speaker_type speaker_text  relation_type relation_text  label  \n",
       "0        1.0             0           PM            5.0  Continuation      0  \n",
       "1        2.0             0           PM            5.0  Continuation      0  \n",
       "2        3.0             0           PM            1.0   Explanation      1  \n",
       "3        4.0             0           PM           10.0   Elaboration      0  \n",
       "4        5.0             0           PM            5.0  Continuation      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concaténer tous les DataFrames en un seul\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "with open(\"data/training_labels.json\", 'r') as file:\n",
    "    labels_data = json.load(file)\n",
    "\n",
    "df['label'] = df.apply(lambda row: get_label(row['dialogue_id'], row['index_start'], labels_data), axis=1)\n",
    "\n",
    "# Afficher le DataFrame final\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bert'] = df['text']\n",
    "for transcription_id in dialogue_ids:\n",
    "    bert_array = np.load('feature_bert/training/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df[df['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array):\n",
    "        df.at[indices[idx-1], 'bert'] = value\n",
    "\n",
    "df_test['bert'] = df_test['text']\n",
    "for transcription_id in dialogue_ids_test:\n",
    "    bert_array_test = np.load('feature_bert/test/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df_test[df_test['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array_test):\n",
    "        df_test.at[indices[idx-1], 'bert'] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dif_start_end'] = df['index_end']-df['index_start']\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "df['nb_long_words'] = df['text'].apply(lambda x: sum(len(word) > 4 for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['dif_start_end'] = df_test['index_end']-df_test['index_start']\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(x.split(' ')))\n",
    "df_test['nb_long_words'] = df_test['text'].apply(lambda x: sum(len(word) > 4 for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['word_count','nb_long_words']]\n",
    "y = df[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Ajuster la forme des données\n",
    "X_train = X_train.values.reshape(-1, len(X.columns))\n",
    "X_test = X_test.values.reshape(-1, len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnau\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neighbors\\_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats du modèle :\n",
      "Accuracy:  0.7507745266781412\n",
      "F1-score:  0.3932282936640965\n",
      "\n",
      "Matrice de confusion :\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         9732         1275\n",
      "Actual 1         2345         1173\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le modèle\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nRésultats du modèle :\")\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1-score: \", f1)\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "conf_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(conf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[['word_count','nb_long_words']]\n",
    "y_train = df[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[['word_count','nb_long_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnau\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neighbors\\_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model k-Nearest Neighbors (kNN)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Ajuster le modèle\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_pred = knn_classifier.predict(X_test)\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire pour stocker les prédictions\n",
    "test_labels_final = {dialogue_id: [] for dialogue_id in dialogue_ids_test}\n",
    "\n",
    "# Parcourir les lignes de df_test\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Obtenez les indices des lignes correspondant au dialogue_id\n",
    "    indices = df_test[df_test['dialogue_id'] == dialogue_id].index\n",
    "    # Ajouter les valeurs de test_label[index] au dictionnaire\n",
    "    test_labels_final[dialogue_id] = test_pred[indices].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_labels_text_knc.json\", \"w\") as file:\n",
    "    json.dump(test_labels_final, file, indent=4)\n",
    "\n",
    "#python make_submission.py --json_path arnaud/test_labels_text_knc.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
