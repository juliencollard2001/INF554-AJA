{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72526 entries, 0 to 72525\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   dialogue_id    72526 non-null  object\n",
      " 1   index_start    72526 non-null  int64 \n",
      " 2   text           72526 non-null  object\n",
      " 3   index_end      72526 non-null  int64 \n",
      " 4   speaker_type   72526 non-null  int64 \n",
      " 5   speaker_text   72526 non-null  object\n",
      " 6   relation_type  72526 non-null  int64 \n",
      " 7   relation_text  72526 non-null  object\n",
      " 8   label          72526 non-null  int64 \n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 5.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>index_start</th>\n",
       "      <th>text</th>\n",
       "      <th>index_end</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>relation_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>10</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>Right</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>10</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>2</td>\n",
       "      <td>Explanation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>3</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>7</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>4</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>10</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id  index_start                                               text  \\\n",
       "0     ES2002a            0                                               Okay   \n",
       "1     ES2002a            1                                              Right   \n",
       "2     ES2002a            2  <vocalsound> Um well this is the kick-off meet...   \n",
       "3     ES2002a            3                             Um <vocalsound> and um   \n",
       "4     ES2002a            4  this is just what we're gonna be doing over th...   \n",
       "\n",
       "   index_end  speaker_type speaker_text  relation_type relation_text  label  \n",
       "0          1             2           PM             10  Continuation      0  \n",
       "1          2             2           PM             10  Continuation      0  \n",
       "2          3             2           PM              2   Explanation      1  \n",
       "3          4             2           PM              7   Elaboration      0  \n",
       "4          5             2           PM             10  Continuation      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read transcription data\n",
    "def read_transcription(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Function to read discourse graph data\n",
    "def read_discourse_graph(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [line.strip().split() for line in file]\n",
    "\n",
    "    # Convert non-numeric values to numeric indices\n",
    "    data = [(int(start), relation, int(end)) if start.isdigit() and end.isdigit() else (start, relation, end) for start, relation, end in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict):\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through all edges in the discourse graph\n",
    "    for edge in discourse_graph:\n",
    "        index_start, relation_type, index_end = edge\n",
    "\n",
    "        # Retrieve speaker information\n",
    "        speaker = transcription[index_start]['speaker']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        speaker_id = speaker_dict.get(speaker, -1)\n",
    "\n",
    "        # Retrieve the sentence\n",
    "        text = transcription[index_start]['text']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        relation_type_id = relation_dict.get(relation_type, -1)\n",
    "\n",
    "        # Add a row to the DataFrame\n",
    "        rows.append({\n",
    "            'dialogue_id': dialogue_id,\n",
    "            'index_start': index_start,\n",
    "            'text': text,\n",
    "            'index_end': index_end,\n",
    "            'speaker_type': speaker_id,\n",
    "            'speaker_text': speaker,\n",
    "            'relation_type': relation_type_id,\n",
    "            'relation_text': relation_type\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to create the relation conversion dictionary\n",
    "def create_relation_dict(discourse_graph):\n",
    "    relation_set = set()\n",
    "\n",
    "    # Collect all unique relations\n",
    "    for edge in discourse_graph:\n",
    "        relation_set.add(edge[1])\n",
    "\n",
    "    # Create a conversion dictionary\n",
    "    relation_dict = {relation: idx for idx, relation in enumerate(relation_set)}\n",
    "\n",
    "    return relation_dict\n",
    "\n",
    "# Function to create the speaker conversion dictionary\n",
    "def create_speaker_dict(transcription):\n",
    "    speaker_set = set()\n",
    "\n",
    "    # Collect all unique speakers\n",
    "    for utterance in transcription:\n",
    "        speaker_set.add(utterance['speaker'])\n",
    "\n",
    "    # Create a conversion dictionary\n",
    "    speaker_dict = {speaker: idx for idx, speaker in enumerate(speaker_set)}\n",
    "\n",
    "    return speaker_dict\n",
    "\n",
    "# Function to get labels for a dialogue\n",
    "def get_label(dialogue_id, index,labels_data):\n",
    "    return labels_data.get(dialogue_id, [])[index]\n",
    "\n",
    "# Function to load BERT embeddings from .npy files\n",
    "def load_bert_embeddings(dialogue_id):\n",
    "    file_path = f'feature-extraction/bert/training/{dialogue_id}.npy'\n",
    "    return np.load(file_path)\n",
    "\n",
    "# Replace 'your_path' with the correct path\n",
    "path_to_training = Path(\"data/training\")\n",
    "path_to_test = Path(\"data/test\")\n",
    "\n",
    "# List to store DataFrames for each dialogue\n",
    "dfs = []\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "# Replace 'your_dialogue_ids' with your actual list of dialogue identifiers\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "# List to store DataFrames for each dialogue\n",
    "dfs = []\n",
    "\n",
    "# Iterate through each dialogue\n",
    "for dialogue_id in training_set:\n",
    "    # Read transcription and discourse graph data\n",
    "    transcription = read_transcription(path_to_training / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_to_training / f'{dialogue_id}.txt')\n",
    "\n",
    "    # Create the relation conversion dictionary\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Create the DataFrame for the current dialogue\n",
    "    df_dialogue = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "\n",
    "    # Add the DataFrame to the list\n",
    "    dfs.append(df_dialogue)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Load the training labels data\n",
    "with open(\"data/training_labels.json\", 'r') as file:\n",
    "    labels_data = json.load(file)\n",
    "    \n",
    "df['label'] = df.apply(lambda row: get_label(row['dialogue_id'], row['index_start'], labels_data), axis=1)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df.info()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30986 entries, 0 to 30985\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   dialogue_id    30986 non-null  object\n",
      " 1   index_start    30986 non-null  int64 \n",
      " 2   text           30986 non-null  object\n",
      " 3   index_end      30986 non-null  int64 \n",
      " 4   speaker_type   30986 non-null  int64 \n",
      " 5   speaker_text   30986 non-null  object\n",
      " 6   relation_type  30986 non-null  int64 \n",
      " 7   relation_text  30986 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 1.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>index_start</th>\n",
       "      <th>text</th>\n",
       "      <th>index_end</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>relation_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES2003a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay , well</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>11</td>\n",
       "      <td>Continuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES2003a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay , well</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>11</td>\n",
       "      <td>Continuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES2003a</td>\n",
       "      <td>2</td>\n",
       "      <td>Right ,</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "      <td>Comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES2003a</td>\n",
       "      <td>3</td>\n",
       "      <td>my name's Adam Duguid ,</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>2</td>\n",
       "      <td>Explanation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES2003a</td>\n",
       "      <td>4</td>\n",
       "      <td>we're here because of real reaction ,</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>2</td>\n",
       "      <td>Explanation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id  index_start                                   text  index_end  \\\n",
       "0     ES2003a            0                            Okay , well          1   \n",
       "1     ES2003a            0                            Okay , well          2   \n",
       "2     ES2003a            2                                Right ,          3   \n",
       "3     ES2003a            3                my name's Adam Duguid ,          4   \n",
       "4     ES2003a            4  we're here because of real reaction ,          5   \n",
       "\n",
       "   speaker_type speaker_text  relation_type relation_text  \n",
       "0             2           PM             11  Continuation  \n",
       "1             2           PM             11  Continuation  \n",
       "2             2           PM              0       Comment  \n",
       "3             2           PM              2   Explanation  \n",
       "4             2           PM              2   Explanation  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
    "\n",
    "# List to store DataFrames for each dialogue\n",
    "dfs_test = []\n",
    "\n",
    "# Iterate through each dialogue\n",
    "for dialogue_id in test_set:\n",
    "    # Read transcription and discourse graph data\n",
    "    transcription = read_transcription(path_to_test / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_to_test / f'{dialogue_id}.txt')\n",
    "\n",
    "    # Create the relation conversion dictionary\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Create the DataFrame for the current dialogue\n",
    "    df_dialogue = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "\n",
    "    # Add the DataFrame to the list\n",
    "    dfs_test.append(df_dialogue)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_test.info()\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dif_start_end'] = df['index_end']-df['index_start']\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "df['nb_long_words'] = df['text'].apply(lambda x: sum(len(word) > 4 for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[['word_count','nb_long_words']]\n",
    "y_train = df[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['dif_start_end'] = df_test['index_end']-df_test['index_start']\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(x.split(' ')))\n",
    "df_test['nb_long_words'] = df_test['text'].apply(lambda x: sum(len(word) > 4 for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[['word_count','nb_long_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [29010, 58020]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arnau\\OneDrive\\Documents\\Scolarité X\\3A\\INF554-AJA\\arnaud\\submisison_a.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/arnaud/submisison_a.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m knn_classifier \u001b[39m=\u001b[39m KNeighborsClassifier()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/arnaud/submisison_a.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Ajuster le modèle\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/arnaud/submisison_a.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m knn_classifier\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/arnaud/submisison_a.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Prédictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arnau/OneDrive/Documents/Scolarit%C3%A9%20X/3A/INF554-AJA/arnaud/submisison_a.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred \u001b[39m=\u001b[39m knn_classifier\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neighbors\\_classification.py:233\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m@_fit_context\u001b[39m(\n\u001b[0;32m    212\u001b[0m     \u001b[39m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m    216\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[0;32m    218\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neighbors\\_base.py:456\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    455\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 456\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    457\u001b[0m             X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    458\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     \u001b[39mif\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m         \u001b[39m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    462\u001b[0m         \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1166\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [29010, 58020]"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Model k-Nearest Neighbors (kNN)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Ajuster le modèle\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = {}\n",
    "for transcription_id in test_set:\n",
    "    test_labels[transcription_id] = y_pred.tolist()\n",
    "\n",
    "with open(\"test_labels_text_baseline.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
