{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lire les données de transcription\n",
    "def read_transcription(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Fonction pour lire les données du graphe de discours\n",
    "def read_discourse_graph(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [line.strip().split() for line in file]\n",
    "\n",
    "    # Convertir les valeurs non numériques en indices numériques\n",
    "    data = [(int(start), relation, int(end)) if start.isdigit() and end.isdigit() else (start, relation, end) for start, relation, end in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict):\n",
    "    rows = []\n",
    "\n",
    "      # Iterate through all edges in the discourse graph\n",
    "    for edge in discourse_graph:\n",
    "        index_start, relation_type, index_end = edge\n",
    "\n",
    "        # Retrieve speaker information\n",
    "        speaker = transcription[index_start]['speaker']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        speaker_id = speaker_dict.get(speaker, -1)\n",
    "\n",
    "        # Retrieve the sentence\n",
    "        text = transcription[index_start]['text']\n",
    "\n",
    "        # Convert relation type to integer using the dictionary\n",
    "        relation_type_id = relation_dict.get(relation_type, -1)\n",
    "\n",
    "        # Add a row to the DataFrame\n",
    "        rows.append({\n",
    "            'dialogue_id': dialogue_id,\n",
    "            'index_start': index_start,\n",
    "            'text': text,\n",
    "            'index_end': index_end,\n",
    "            'speaker_type': speaker_id,\n",
    "            'speaker_text': speaker,\n",
    "            'relation_type': relation_type_id,\n",
    "            'relation_text': relation_type\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des relations\n",
    "def create_relation_dict(discourse_graph):\n",
    "    relation_set = set()\n",
    "\n",
    "    # Collecter toutes les relations uniques\n",
    "    for edge in discourse_graph:\n",
    "        relation_set.add(edge[1])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    relation_dict = {relation: idx for idx, relation in enumerate(relation_set)}\n",
    "\n",
    "    return relation_dict\n",
    "\n",
    "# Fonction pour créer le dictionnaire de conversion des speakers\n",
    "def create_speaker_dict(transcription):\n",
    "    speaker_set = set()\n",
    "\n",
    "    # Collecter tous les locuteurs uniques\n",
    "    for utterance in transcription:\n",
    "        speaker_set.add(utterance['speaker'])\n",
    "\n",
    "    # Créer un dictionnaire de conversion\n",
    "    speaker_dict = {speaker: idx for idx, speaker in enumerate(speaker_set)}\n",
    "\n",
    "    return speaker_dict\n",
    "\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "# Function to get labels for a dialogue\n",
    "def get_label(dialogue_id, index,labels_data):\n",
    "    return labels_data.get(dialogue_id, [])[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacez 'votre_chemin' par le chemin correct\n",
    "path_train= Path(\"data/training\")\n",
    "path_test= Path(\"data/test\")\n",
    "\n",
    "# Remplacez 'vos_dialogue_ids' par votre liste réelle d'identifiants de dialogue\n",
    "dialogue_ids = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "dialogue_ids = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids])\n",
    "dialogue_ids.remove('IS1002a')\n",
    "dialogue_ids.remove('IS1005d')\n",
    "dialogue_ids.remove('TS3012c')\n",
    "\n",
    "dialogue_ids_test = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "dialogue_ids_test = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in dialogue_ids_test])\n",
    "\n",
    "# Liste pour stocker les DataFrames de chaque dialogue\n",
    "dfs = []\n",
    "dfs_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_train / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_train / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs.append(df)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': np.nan,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': np.nan,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs.append(pd.DataFrame([last_row]))\n",
    "\n",
    "# Parcourir chaque dialogue\n",
    "for dialogue_id in dialogue_ids_test:\n",
    "    # Lire les données de transcription et de graphe de discours\n",
    "    transcription = read_transcription(path_test / f'{dialogue_id}.json')\n",
    "    discourse_graph = read_discourse_graph(path_test / f'{dialogue_id}.txt')\n",
    "    \n",
    "    # Créer le dictionnaire de conversion des relations\n",
    "    relation_dict = create_relation_dict(discourse_graph)\n",
    "    speaker_dict = create_speaker_dict(transcription)\n",
    "\n",
    "    # Créer le DataFrame pour le dialogue actuel\n",
    "    df_test = create_dataframe(dialogue_id, transcription, discourse_graph, relation_dict, speaker_dict)\n",
    "    \n",
    "    # Ajouter le DataFrame à la liste\n",
    "    dfs_test.append(df_test)\n",
    "\n",
    "    # Ajouter la dernière phrase avec NaN pour index_end et 'relation'\n",
    "    last_utterance = transcription[-1]\n",
    "    last_speaker = last_utterance['speaker']\n",
    "    last_text = last_utterance['text']\n",
    "    last_row = {\n",
    "        'dialogue_id': dialogue_id,\n",
    "        'index_start': len(transcription) - 1,\n",
    "        'text': last_text,\n",
    "        'index_end': np.nan,\n",
    "        'speaker_type': speaker_dict.get(last_speaker, -1),\n",
    "        'speaker_text': last_speaker,\n",
    "        'relation_type': np.nan,\n",
    "        'relation_text': np.nan\n",
    "    }\n",
    "    dfs_test.append(pd.DataFrame([last_row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer tous les DataFrames en un seul\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "with open(\"data/training_labels.json\", 'r') as file:\n",
    "    labels_data = json.load(file)\n",
    "\n",
    "df['label'] = df.apply(lambda row: get_label(row['dialogue_id'], row['index_start'], labels_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bert'] = df['text']\n",
    "for transcription_id in dialogue_ids:\n",
    "    bert_array = np.load('feature_bert/training/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df[df['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array):\n",
    "        df.at[indices[idx-1], 'bert'] = value\n",
    "\n",
    "df_test['bert'] = df_test['text']\n",
    "for transcription_id in dialogue_ids_test:\n",
    "    bert_array_test = np.load('feature_bert/test/' + transcription_id + '.npy')\n",
    "    \n",
    "    # Obtenez les indices des lignes correspondant à la transcription_id\n",
    "    indices = df_test[df_test['dialogue_id'] == transcription_id].index\n",
    "    \n",
    "    # Remplacez les valeurs de la colonne 'text' par les valeurs de bert_array\n",
    "    for idx, value in enumerate(bert_array_test):\n",
    "        df_test.at[indices[idx-1], 'bert'] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'éléments dans chaque liste\n",
    "num_elements = len(df['bert'].iloc[0])\n",
    "\n",
    "# Créez de nouvelles colonnes pour chaque élément dans la liste\n",
    "new_columns = [f'coord_{i}' for i in range(num_elements)]\n",
    "\n",
    "# Appliquez une fonction qui divise chaque liste en plusieurs colonnes\n",
    "new_text_columns = df['bert'].apply(pd.Series)\n",
    "\n",
    "# Renommez les nouvelles colonnes avec les noms spécifiques\n",
    "new_text_columns.columns = new_columns\n",
    "\n",
    "# Concaténez les nouvelles colonnes avec le DataFrame existant\n",
    "df = pd.concat([df, new_text_columns], axis=1)\n",
    "\n",
    "\n",
    "# Nombre d'éléments dans chaque liste\n",
    "num_elements = len(df_test['bert'].iloc[0])\n",
    "\n",
    "# Appliquez une fonction qui divise chaque liste en plusieurs colonnes\n",
    "new_text_columns_test = df_test['bert'].apply(pd.Series)\n",
    "\n",
    "# Renommez les nouvelles colonnes avec les noms spécifiques\n",
    "new_text_columns_test.columns = new_columns\n",
    "\n",
    "# Concaténez les nouvelles colonnes avec le DataFrame existant\n",
    "df_test = pd.concat([df_test, new_text_columns_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>index_start</th>\n",
       "      <th>text</th>\n",
       "      <th>index_end</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>relation_text</th>\n",
       "      <th>label</th>\n",
       "      <th>bert</th>\n",
       "      <th>...</th>\n",
       "      <th>coord_374</th>\n",
       "      <th>coord_375</th>\n",
       "      <th>coord_376</th>\n",
       "      <th>coord_377</th>\n",
       "      <th>coord_378</th>\n",
       "      <th>coord_379</th>\n",
       "      <th>coord_380</th>\n",
       "      <th>coord_381</th>\n",
       "      <th>coord_382</th>\n",
       "      <th>coord_383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.054861926, 0.047606602, -0.032625835, -0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092259</td>\n",
       "      <td>0.034839</td>\n",
       "      <td>-0.021490</td>\n",
       "      <td>0.007297</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.027128</td>\n",
       "      <td>0.145950</td>\n",
       "      <td>0.037911</td>\n",
       "      <td>0.073511</td>\n",
       "      <td>0.079932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>Right</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.054665036, -0.073837034, -0.017160872, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>-0.025984</td>\n",
       "      <td>0.077994</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.032260</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.059096</td>\n",
       "      <td>-0.036019</td>\n",
       "      <td>-0.008820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Explanation</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.010415948, -0.072719134, -0.017205587, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>0.032185</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.041298</td>\n",
       "      <td>-0.018026</td>\n",
       "      <td>0.050856</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.077368</td>\n",
       "      <td>-0.037393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>3</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.028653603, -0.01515074, 0.095909655, -0.05...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108833</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>-0.011521</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.117780</td>\n",
       "      <td>-0.017561</td>\n",
       "      <td>-0.028903</td>\n",
       "      <td>0.007401</td>\n",
       "      <td>-0.005552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES2002a</td>\n",
       "      <td>4</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Continuation</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.028386272, -0.046021443, 0.023957256, -0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111571</td>\n",
       "      <td>-0.023915</td>\n",
       "      <td>-0.037931</td>\n",
       "      <td>0.040358</td>\n",
       "      <td>0.079421</td>\n",
       "      <td>-0.018038</td>\n",
       "      <td>0.041037</td>\n",
       "      <td>0.034134</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>-0.012039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialogue_id  index_start                                               text  \\\n",
       "0     ES2002a            0                                               Okay   \n",
       "1     ES2002a            1                                              Right   \n",
       "2     ES2002a            2  <vocalsound> Um well this is the kick-off meet...   \n",
       "3     ES2002a            3                             Um <vocalsound> and um   \n",
       "4     ES2002a            4  this is just what we're gonna be doing over th...   \n",
       "\n",
       "   index_end  speaker_type speaker_text  relation_type relation_text  label  \\\n",
       "0        1.0             3           PM           12.0  Continuation      0   \n",
       "1        2.0             3           PM           12.0  Continuation      0   \n",
       "2        3.0             3           PM            5.0   Explanation      1   \n",
       "3        4.0             3           PM            1.0   Elaboration      0   \n",
       "4        5.0             3           PM           12.0  Continuation      0   \n",
       "\n",
       "                                                bert  ...  coord_374  \\\n",
       "0  [-0.054861926, 0.047606602, -0.032625835, -0.0...  ...   0.092259   \n",
       "1  [-0.054665036, -0.073837034, -0.017160872, -0....  ...   0.035382   \n",
       "2  [-0.010415948, -0.072719134, -0.017205587, -0....  ...   0.006533   \n",
       "3  [-0.028653603, -0.01515074, 0.095909655, -0.05...  ...   0.108833   \n",
       "4  [-0.028386272, -0.046021443, 0.023957256, -0.0...  ...   0.111571   \n",
       "\n",
       "   coord_375  coord_376  coord_377  coord_378  coord_379  coord_380  \\\n",
       "0   0.034839  -0.021490   0.007297   0.027587   0.027128   0.145950   \n",
       "1   0.098955  -0.025984   0.077994   0.003580   0.032260   0.022304   \n",
       "2   0.032185   0.010955   0.041298  -0.018026   0.050856   0.007696   \n",
       "3   0.061266  -0.011521  -0.010543   0.010692   0.117780  -0.017561   \n",
       "4  -0.023915  -0.037931   0.040358   0.079421  -0.018038   0.041037   \n",
       "\n",
       "   coord_381  coord_382  coord_383  \n",
       "0   0.037911   0.073511   0.079932  \n",
       "1   0.059096  -0.036019  -0.008820  \n",
       "2   0.041694   0.077368  -0.037393  \n",
       "3  -0.028903   0.007401  -0.005552  \n",
       "4   0.034134   0.028470  -0.012039  \n",
       "\n",
       "[5 rows x 394 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sélection des colonnes pertinentes\n",
    "selected_columns = ['text', 'label']  # Ajoutez d'autres colonnes si nécessaire\n",
    "data = df[selected_columns]\n",
    "\n",
    "# Séparation en variables explicatives (X) et cible (y)\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Régression logistique :\n",
      "Accuracy: 0.8024784853700516\n",
      "f1_score : 0.4978120077017329\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88     11009\n",
      "           1       0.65      0.40      0.50      3516\n",
      "\n",
      "    accuracy                           0.80     14525\n",
      "   macro avg       0.74      0.67      0.69     14525\n",
      "weighted avg       0.79      0.80      0.79     14525\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnau\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report,f1_score\n",
    "\n",
    "# Création du pipeline avec TF-IDF et régression logistique\n",
    "model_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Régression logistique :\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"f1_score :\",f1_score(y_test,y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with TF-IDF:\n",
      "Accuracy: 0.8882616179001721\n",
      "f1_score : 0.7188636757318553\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     11009\n",
      "           1       0.92      0.59      0.72      3516\n",
      "\n",
      "    accuracy                           0.89     14525\n",
      "   macro avg       0.90      0.79      0.82     14525\n",
      "weighted avg       0.89      0.89      0.88     14525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Création du pipeline avec TF-IDF et Random Forest\n",
    "model_rf_tfidf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_rf_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred_rf_tfidf = model_rf_tfidf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Random Forest with TF-IDF:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf_tfidf))\n",
    "print(\"f1_score :\",f1_score(y_test,y_pred_rf_tfidf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf_tfidf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
