{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "import difflib\n",
    "\n",
    "import sys\n",
    "sys.path.append('../AJA')\n",
    "import AJA as aja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données \n",
    "df_train_nodes, df_train_edges, df_test_nodes, df_test_edges = aja.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# node\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# sentence length normalized\n",
    "df_train_nodes['sentence_length'] = df_train_nodes['text'].apply(lambda s: len(s.split()))\n",
    "df_train_nodes['sentence_length'] = scaler.fit_transform(df_train_nodes['sentence_length'].values.reshape(-1, 1))\n",
    "df_test_nodes['sentence_length'] = df_test_nodes['text'].apply(lambda s: len(s.split()))\n",
    "df_test_nodes['sentence_length'] = scaler.transform(df_test_nodes['sentence_length'].values.reshape(-1, 1))\n",
    "\n",
    "df_train_nodes['nb_occurences'] = df_train_nodes['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_train_nodes['nb_occurences'] = scaler.fit_transform(df_train_nodes['nb_occurences'].values.reshape(-1, 1))\n",
    "df_test_nodes['nb_occurences'] = df_test_nodes['text'].apply(lambda x: sum(x.split().count(mot) for mot in ['uh', 'um', 'okay', '<', 'ah', 'oh']))\n",
    "df_test_nodes['nb_occurences'] = scaler.transform(df_test_nodes['nb_occurences'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "df_train_nodes['nb_words_more_5'] = df_train_nodes['text'].apply(lambda x: sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_train_nodes['nb_words_more_5'] = scaler.fit_transform(df_train_nodes['nb_words_more_5'].values.reshape(-1, 1))\n",
    "df_test_nodes['nb_words_more_5'] = df_test_nodes['text'].apply(lambda x: sum(len(mot) > 5 and mot.lower() != '<vocalsound>' for mot in x.split()))\n",
    "df_test_nodes['nb_words_more_5'] = scaler.transform(df_test_nodes['nb_words_more_5'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# speaker hot-one encoding\n",
    "one_hot_encoded = pd.get_dummies(df_train_nodes['speaker_int'], prefix='speaker', dtype=int)\n",
    "df_train_nodes = df_train_nodes.drop('speaker_int', axis=1)\n",
    "df_train_nodes = df_train_nodes.drop('speaker_text', axis=1)\n",
    "df_train_nodes = pd.concat([df_train_nodes, one_hot_encoded], axis=1)\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(df_test_nodes['speaker_int'], prefix='speaker', dtype=int)\n",
    "df_test_nodes = df_test_nodes.drop('speaker_int', axis=1)\n",
    "df_test_nodes = df_test_nodes.drop('speaker_text', axis=1)\n",
    "df_test_nodes = pd.concat([df_test_nodes, one_hot_encoded], axis=1)\n",
    "\n",
    "# edge\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "        'transcription': df_train_edges['transcription'],\n",
    "        'start': df_train_edges['end'],\n",
    "        'end': df_train_edges['start'],\n",
    "        'type_int': 16 + df_train_edges['type_int'],\n",
    "        'type_text': df_train_edges['type_text'] + \"_reverse\"\n",
    "    })\n",
    "df_train_edges = pd.concat([df_train_edges, new_df], ignore_index=True)\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "        'transcription': df_test_edges['transcription'],\n",
    "        'start': df_test_edges['end'],\n",
    "        'end': df_test_edges['start'],\n",
    "        'type_int': 16 + df_test_edges['type_int'],\n",
    "        'type_text': df_test_edges['type_text'] + \"_reverse\"\n",
    "    })\n",
    "df_test_edges = pd.concat([df_test_edges, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'extension autoreload\n",
    "%load_ext autoreload\n",
    "\n",
    "# Configurer autoreload pour recharger tous les modules avant l'exécution de chaque cellule\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des graphs\n",
    "train_graphs, test_graphs = aja.make_graphs(df_train_nodes, df_train_edges, df_test_nodes, df_test_edges)\n",
    "N_features = train_graphs['ES2002a'].x.shape[1]\n",
    "train_graphs, validation_graphs = aja.train_validation_split(train_graphs, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelsGCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, input_dim, post_conv_dim, output_dim, identity=False):\n",
    "        super(MultiChannelsGCN, self).__init__()\n",
    "        self.identity = identity\n",
    "        self.channels = channels\n",
    "        self.input_dim = input_dim\n",
    "        self.post_conv_dim = post_conv_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.GCN = nn.ModuleList([GCNConv(input_dim, post_conv_dim) for _ in range(channels)])\n",
    "        if identity:\n",
    "            self.dense = nn.Linear(post_conv_dim * (channels + 1), output_dim)\n",
    "            self.denseID = nn.Linear(input_dim, post_conv_dim)\n",
    "        else:\n",
    "            self.dense = nn.Linear(post_conv_dim * channels, output_dim)\n",
    "\n",
    "    def forward(self, nodes, edges):\n",
    "        X = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.post_conv_dim)\n",
    "            else:\n",
    "                x = F.relu(self.GCN[k](nodes, edges[k]))\n",
    "            X.append(x)\n",
    "        if self.identity:\n",
    "            X.append(F.relu(self.denseID(nodes)))\n",
    "        concat = torch.cat(X, dim=1)\n",
    "        return F.relu(self.dense(concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définie son plus beau modèle\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.GCN1 = MultiChannelsGCN(channels, input_dim, 50, 20, identity=True)\n",
    "        self.dense1 = nn.Linear(20,1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "        x = self.GCN1(nodes, edges)\n",
    "        x = self.dense1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, graph):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(graph)\n",
    "        return np.array((logits > 0.5).int()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import torch.nn as nn\n",
    "\n",
    "alpha = 0.2\n",
    "gamma = 5\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((1 - alpha) / alpha), reduction='mean')\n",
    "\n",
    "# call backs\n",
    "checkpoint = Checkpoint(monitor='valid_f1_score', dirname='training_states/skorch_best_model')\n",
    "lrscheduler = LRScheduler(monitor='valid_f1_score', policy='ReduceLROnPlateau', factor=0.5, patience=5)\n",
    "early_stopping = EarlyStopping(monitor='valid_f1_score', patience=5, threshold=0.001)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    NodeClassifier(channels=32, input_dim=N_features),              \n",
    "    criterion=criterion,    \n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint, lrscheduler, early_stopping],\n",
    "    lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No y-values are given (y=None). You must either supply a Dataset as X or implement your own DataLoader for training (and your validation) and supply it using the ``iterator_train`` and ``iterator_valid`` parameters respectively.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/INF554-AJA/julien/skorch.ipynb Cellule 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bjupyter-server-rv947rxjg5j3pv9v/workspaces/INF554-AJA/julien/skorch.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net\u001b[39m.\u001b[39;49mfit(train_graphs, y\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, valid_graphs\u001b[39m=\u001b[39;49mvalidation_graphs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(NeuralNetClassifier, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_train_begin\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1279\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/skorch/net.py:1172\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_loop\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, epochs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[1;32m   1137\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The proper fit loop.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[39m    Contains the logic of what actually happens during the fit\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \n\u001b[1;32m   1171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_data(X, y)\n\u001b[1;32m   1173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_training_readiness()\n\u001b[1;32m   1174\u001b[0m     epochs \u001b[39m=\u001b[39m epochs \u001b[39mif\u001b[39;00m epochs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/skorch/classifier.py:129\u001b[0m, in \u001b[0;36mNeuralNetClassifier.check_data\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    120\u001b[0m         (y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         (\u001b[39mnot\u001b[39;00m is_dataset(X)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator_train \u001b[39mis\u001b[39;00m DataLoader)\n\u001b[1;32m    123\u001b[0m ):\n\u001b[1;32m    124\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mNo y-values are given (y=None). You must either supply a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mDataset as X or implement your own DataLoader for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mtraining (and your validation) and supply it using the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39m``iterator_train`` and ``iterator_valid`` parameters \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mrespectively.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m (y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m is_dataset(X):\n\u001b[1;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: No y-values are given (y=None). You must either supply a Dataset as X or implement your own DataLoader for training (and your validation) and supply it using the ``iterator_train`` and ``iterator_valid`` parameters respectively."
     ]
    }
   ],
   "source": [
    "net.fit(train_graphs, y=None, valid_graphs=validation_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Epoch 000 -\n",
      "Loss: 1.0241\n",
      "F1 train: 0.589312645181491\n",
      "F1 valid: 0.5614600504414725\n",
      "It's better !\n",
      "Epoch time: 2.6176323890686035 seconds\n",
      "\n",
      "- Epoch 001 -\n",
      "Loss: 0.9900\n",
      "F1 train: 0.6025812309907801\n",
      "F1 valid: 0.5586991840191596\n",
      "It's better !\n",
      "Epoch time: 2.620648145675659 seconds\n",
      "\n",
      "- Epoch 002 -\n",
      "Loss: 0.9848\n",
      "F1 train: 0.6136371984829395\n",
      "F1 valid: 0.5551722238864718\n",
      "It's better !\n",
      "Epoch time: 2.5921075344085693 seconds\n",
      "\n",
      "- Epoch 003 -\n",
      "Loss: 0.9774\n",
      "F1 train: 0.6152101265492641\n",
      "F1 valid: 0.5503814405976211\n",
      "It's better !\n",
      "Epoch time: 2.5995893478393555 seconds\n",
      "\n",
      "- Epoch 004 -\n",
      "Loss: 0.9762\n",
      "F1 train: 0.6221293923761706\n",
      "F1 valid: 0.5541519665404954\n",
      "It's better !\n",
      "Epoch time: 2.8570423126220703 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the instantiation of the model outside the training loop\n",
    "model = NodeClassifier(32, N_features)\n",
    "model_name='test_base'\n",
    "\n",
    "# Move the model and data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Use DataLoader to create batches of data\n",
    "train_loader = DataLoader(list(train_graphs.values()), batch_size=1, shuffle=True)\n",
    "N_train = len(train_loader)\n",
    "validation_loader = DataLoader(list(validation_graphs.values()), batch_size=1, shuffle=False)\n",
    "N_validation = len(validation_loader)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "alpha = 0.2\n",
    "gamma = 5\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((1 - alpha) / alpha), reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Nombre d'epochs\n",
    "epochs = 5\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "# Train the model\n",
    "best_f1_score = 0\n",
    "for epoch in range(epochs):\n",
    "    print('- Epoch', f'{epoch:03d}', '-')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / N_train\n",
    "    print('Loss:', f'{average_loss:.4f}')\n",
    "    \n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    f1_moyen_train = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        y_pred = model.predict(data)\n",
    "        y_true = data.y.cpu().numpy()\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        f1_moyen_train += f1\n",
    "    f1_moyen_train /= N_train\n",
    "    print('F1 train:', f1_moyen_train)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    f1_moyen_valid = 0\n",
    "    for data in validation_loader:\n",
    "        data = data.to(device)\n",
    "        y_pred = model.predict(data)\n",
    "        y_true = data.y.cpu().numpy()\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        f1_moyen_valid += f1\n",
    "    f1_moyen_valid /= N_validation\n",
    "    print('F1 valid:', f1_moyen_valid)\n",
    "\n",
    "    # callbacks ou autre\n",
    "    if f1_moyen_valid > best_f1_score:\n",
    "        print('It\\'s better !' )\n",
    "        torch.save(model.state_dict(), \"training_states/\" + model_name + \"-best.pth\")\n",
    "    else:\n",
    "        optimizer.param_groups[0]['lr'] /= 2\n",
    "        print('Learning rate reduced to:', optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print('Epoch time:', epoch_time, 'seconds')\n",
    "    print('')\n",
    "\n",
    "model.load_state_dict(torch.load(\"training_states/\" + model_name + \"-best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
