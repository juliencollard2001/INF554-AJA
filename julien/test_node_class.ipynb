{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "path_to_training = Path(\"data/training\")\n",
    "path_to_test = Path(\"data/test\")\n",
    "\n",
    "#####\n",
    "# training and test sets of transcription ids\n",
    "#####\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "graph_links_labels= set()\n",
    "for id in training_set:\n",
    "    with open(path_to_training / f\"{id}.txt\", \"r\") as graphe:\n",
    "        for line in graphe:\n",
    "            l = line.split()\n",
    "            graph_links_labels.add(l[1])\n",
    "L = list(graph_links_labels)\n",
    "int2label = {indice: valeur for indice, valeur in enumerate(L)}\n",
    "label2int = {valeur: indice for indice, valeur in enumerate(L)}\n",
    "label2int\n",
    "N_vocab_links = len(L)\n",
    "print(N_vocab_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training():\n",
    "\n",
    "    N_files = len(training_set)\n",
    "    D_embedding = 384\n",
    "\n",
    "    graphs = [] \n",
    "\n",
    "    # lets got throug all training graphs\n",
    "    for k, transcription_id in enumerate(training_set):\n",
    "        #nodes\n",
    "        bert_array = np.load('feature-extraction/bert/training/' + transcription_id +'.npy')\n",
    "        x = torch.tensor(bert_array.reshape(-1,384), dtype=torch.float)\n",
    "        #edges\n",
    "        edges = [[] for _ in range(16)]\n",
    "        with open(path_to_training / f\"{transcription_id}.txt\", \"r\") as graphe:\n",
    "            for line in graphe:\n",
    "                l = line.split()\n",
    "                i = int(l[0])\n",
    "                j =  int(l[2])\n",
    "                edge_type = label2int[l[1]] - 1\n",
    "                edges[edge_type].append([i,j])\n",
    "        edges = [torch.tensor(edges[k]).t().contiguous() for k in range(16)]\n",
    "        #labels\n",
    "        with open(\"data/training_labels.json\", \"r\") as file:\n",
    "            training_labels = json.load(file)\n",
    "        labels = torch.tensor(np.array(training_labels[transcription_id]))\n",
    "        graph = Data(x=x, edge_index=edges, y=labels)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "def extract_test():\n",
    "\n",
    "    N_files = len(training_set)\n",
    "    D_embedding = 384\n",
    "\n",
    "    graphs = [] \n",
    "\n",
    "    # lets got throug all training graphs\n",
    "    for k, transcription_id in enumerate(test_set):\n",
    "        #nodes\n",
    "        bert_array = np.load('feature-extraction/bert/test/' + transcription_id +'.npy')\n",
    "        x = torch.tensor(bert_array.reshape(-1,384), dtype=torch.float)\n",
    "        #edges\n",
    "        edges = [[] for _ in range(16)]\n",
    "        with open(path_to_test / f\"{transcription_id}.txt\", \"r\") as graphe:\n",
    "            for line in graphe:\n",
    "                l = line.split()\n",
    "                i = int(l[0])\n",
    "                j =  int(l[2])\n",
    "                edge_type = label2int[l[1]] - 1\n",
    "                edges[edge_type].append([i,j])\n",
    "        edges = [torch.tensor(edges[k]).t().contiguous() for k in range(16)]\n",
    "        \n",
    "        graph = Data(x=x, edge_index=edges)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "data = extract_training()\n",
    "\n",
    "data_test_kagle = extract_test()\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "data_train = data[:-nb_test]\n",
    "data_test = data[-nb_test:]\n",
    "train_loader = DataLoader(data_train)\n",
    "test_loader = DataLoader(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.sc1 = 100\n",
    "        self.f1 = 50\n",
    "        self.sc2 = 30\n",
    "        self.GCN1 = nn.ModuleList([GCNConv(input_dim, self.sc1) for _ in range(channels)])\n",
    "        self.dense1 = nn.Linear(self.sc1*channels, self.f1)\n",
    "        self.GCN2 = nn.ModuleList([GCNConv(self.f1, self.sc2) for _ in range(channels)])\n",
    "        self.dense2 = nn.Linear(self.sc2*channels, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "        # Appliquez les couches GCN avec une activation ReLU entre elles\n",
    "        x1 = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.sc1)\n",
    "            else:\n",
    "                x = F.relu(self.GCN1[k](nodes, edges[k]))\n",
    "            x1.append(x)\n",
    "        x1_f = torch.cat(x1, dim=1)\n",
    "\n",
    "        f1 = F.relu(self.dense1(x1_f))\n",
    "\n",
    "        x2 = []\n",
    "        for k in range(self.channels):\n",
    "            if len(edges[k]) == 0:\n",
    "                x = torch.zeros(nodes.shape[0], self.sc2)\n",
    "            else:\n",
    "                x = F.relu(self.GCN2[k](f1, edges[k]))\n",
    "            x1.append(x)\n",
    "            x2.append(x)\n",
    "        x2_f = torch.cat(x2, dim=1)\n",
    "\n",
    "        x_out = self.dense2(x2_f)\n",
    "\n",
    "        return F.log_softmax(x_out, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def f1_score(y_pred, y_real):\n",
    "    conf_matrix = confusion_matrix(y_real, y_pred)\n",
    "    tp, fp, fn, tn = conf_matrix[1, 1], conf_matrix[0, 1], conf_matrix[1, 0], conf_matrix[0, 0]\n",
    "    if (tp + fp) == 0:\n",
    "        return 0\n",
    "    if (tp + fn) == 0:\n",
    "        return 0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Data\n",
      "13292 labels 1 sur  72623 noeuds soit 18.30274155570549 %\n",
      "-----------------------------\n",
      "Training set : 77 graphs\n",
      "10580 labels 1 sur  53802 noeuds soit 19.66469647968477 %\n",
      "Testing set : 20\n",
      "2712 labels 1 sur  18821 noeuds soit 14.409436267998512 %\n",
      "-----------------------------\n",
      "Epoch: 000, Loss: 30.3074\n",
      "F1-score: 0.4852945796766178\n",
      "Epoch: 001, Loss: 26.7714\n",
      "F1-score: 0.4991504385434709\n",
      "Epoch: 002, Loss: 25.7874\n",
      "F1-score: 0.49952557928925917\n",
      "Epoch: 003, Loss: 24.8037\n",
      "F1-score: 0.501269095472705\n",
      "-----------------------------\n",
      "Test du modèle :\n",
      "5112 label 1 prédits sur les  2712 voulus ( 27.16114977950162 %)\n"
     ]
    }
   ],
   "source": [
    "# Instanciez le modèle\n",
    "model = NodeClassifier(16,384)\n",
    "\n",
    "# Définissez la fonction de perte et l'optimiseur\n",
    "f = 0.3\n",
    "#criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1-f, f]))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraînez le modèle\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_tot = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y.long())  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        loss_tot += loss\n",
    "    return loss_tot\n",
    "\n",
    "def test_during_training():\n",
    "    model.eval()\n",
    "    S = 0\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        #print(predicted.numpy())\n",
    "        f1 = f1_score(predicted.numpy(), data.y.numpy())\n",
    "        S += f1\n",
    "    f1_moyen = S / len(test_loader)\n",
    "    f1_naive = f1_score(np.ones(len(predicted), dtype=int), data.y.numpy())\n",
    "    print(f'F1-score: {f1_moyen}')\n",
    "\n",
    "def prediction(graph):\n",
    "    model.eval()\n",
    "    out = model(graph)\n",
    "    _, predicted = torch.max(out, 1)\n",
    "    return predicted.numpy()\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print('Data')\n",
    "ones = sum([np.sum(g.y.numpy()) for g in data])\n",
    "tot_nodes = sum([g.x.numpy().shape[0] for g in data])\n",
    "print(ones, 'labels 1 sur ',tot_nodes,'noeuds soit', 100*ones/tot_nodes, '%')\n",
    "print(\"-----------------------------\")\n",
    "print('Training set :', len(data_train),'graphs')\n",
    "ones = sum([np.sum(g.y.numpy()) for g in data_train])\n",
    "tot_nodes = sum([g.x.numpy().shape[0] for g in data_train])\n",
    "print(ones, 'labels 1 sur ',tot_nodes,'noeuds soit', 100*ones/tot_nodes, '%')\n",
    "print('Testing set :', len(data_test))\n",
    "ones_test = sum([np.sum(g.y.numpy()) for g in data_test])\n",
    "tot_nodes_test = sum([g.x.numpy().shape[0] for g in data_test])\n",
    "print(ones_test, 'labels 1 sur ',tot_nodes_test,'noeuds soit', 100*ones_test/tot_nodes_test, '%')\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "for epoch in range(4):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    test_during_training()\n",
    "\n",
    "ones_predicted = sum([np.sum(prediction(g)) for g in test_loader])\n",
    "print(\"-----------------------------\")\n",
    "print(\"Test du modèle :\")\n",
    "print(ones_predicted, 'label 1 prédits sur les ',ones_test,'voulus (',100 * ones_predicted/tot_nodes_test,'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ES2003a', 'ES2003b', 'ES2003c', 'ES2003d', 'ES2004a', 'ES2004b', 'ES2004c', 'ES2004d', 'ES2011a', 'ES2011b', 'ES2011c', 'ES2011d', 'ES2014a', 'ES2014b', 'ES2014c', 'ES2014d', 'IS1008a', 'IS1008b', 'IS1008c', 'IS1008d', 'IS1009a', 'IS1009b', 'IS1009c', 'IS1009d', 'TS3003a', 'TS3003b', 'TS3003c', 'TS3003d', 'TS3004a', 'TS3004b', 'TS3004c', 'TS3004d', 'TS3006a', 'TS3006b', 'TS3006c', 'TS3006d', 'TS3007a', 'TS3007b', 'TS3007c', 'TS3007d']\n",
      "ES2003a\n",
      "ES2003b\n",
      "ES2003c\n",
      "ES2003d\n",
      "ES2004a\n",
      "ES2004b\n",
      "ES2004c\n",
      "ES2004d\n",
      "ES2011a\n",
      "ES2011b\n",
      "ES2011c\n",
      "ES2011d\n",
      "ES2014a\n",
      "ES2014b\n",
      "ES2014c\n",
      "ES2014d\n",
      "IS1008a\n",
      "IS1008b\n",
      "IS1008c\n",
      "IS1008d\n",
      "IS1009a\n",
      "IS1009b\n",
      "IS1009c\n",
      "IS1009d\n",
      "TS3003a\n",
      "TS3003b\n",
      "TS3003c\n",
      "TS3003d\n",
      "TS3004a\n",
      "TS3004b\n",
      "TS3004c\n",
      "TS3004d\n",
      "TS3006a\n",
      "TS3006b\n",
      "TS3006c\n",
      "TS3006d\n",
      "TS3007a\n",
      "TS3007b\n",
      "TS3007c\n",
      "TS3007d\n"
     ]
    }
   ],
   "source": [
    "test_labels = {}\n",
    "print(test_set)\n",
    "for i, graph in enumerate(data_test_kagle):\n",
    "    id = test_set[i]\n",
    "    print(id)\n",
    "    y_test = prediction(graph)\n",
    "    test_labels[id] = y_test.tolist()\n",
    "\n",
    "with open(\"test_labels_GNN.json\", \"w\") as file:\n",
    "    json.dump(test_labels, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n",
      "tensor([0, 1, 0])\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Créez un graphe simple avec des fonctionnalités pour chaque nœud\n",
    "edge_index_1 = torch.tensor([[0, 1], \n",
    "                           [1, 2]], dtype=torch.long)\n",
    "edge_index_2 = torch.tensor([[0, 1], \n",
    "                           [2, 2]], dtype=torch.long)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float)\n",
    "\n",
    "# Définissez les étiquettes de classe pour chaque nœud\n",
    "y = torch.tensor([0, 1, 0], dtype=torch.long)\n",
    "\n",
    "# Créez un objet Data pour stocker le graphe\n",
    "data = Data(x=x, edge_index=[edge_index_1, edge_index_2], y=y)\n",
    "\n",
    "train_dataset = [data, data]\n",
    "\n",
    "# Définissez le modèle GCN\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.conv1 = GCNConv(2, 2)  # 2 features in, 16 features out\n",
    "        self.conv2 = GCNConv(2, 2)  # 16 features in, 2 features out\n",
    "        self.fc = nn.Linear(2 * 2, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Appliquez les couches GCN avec une activation ReLU entre elles\n",
    "        x1 = self.conv1(x, edge_index[0])\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = self.conv2(x, edge_index[1])\n",
    "        x2 = F.relu(x2)\n",
    "\n",
    "        x_cat = torch.cat([x1, x2], dim=1)\n",
    "\n",
    "        x_out = self.fc(x_cat)\n",
    "\n",
    "        return F.log_softmax(x_out, dim=1)\n",
    "\n",
    "# Instanciez le modèle\n",
    "model = GCNNet()\n",
    "\n",
    "# Définissez la fonction de perte et l'optimiseur\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(train_dataset)\n",
    "# Entraînez le modèle\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        print(predicted)\n",
    "        total += data.y.size(0)\n",
    "        correct += (predicted == data.y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0])\n",
      "tensor([0, 1, 0])\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Créez un graphe simple avec des fonctionnalités pour chaque nœud\n",
    "edge_index_1 = torch.tensor([[0, 1], \n",
    "                           [1, 2]], dtype=torch.long)\n",
    "edge_index_2 = torch.tensor([[0, 1], \n",
    "                           [2, 2]], dtype=torch.long)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float)\n",
    "\n",
    "# Définissez les étiquettes de classe pour chaque nœud\n",
    "y = torch.tensor([0, 1, 0], dtype=torch.long)\n",
    "\n",
    "# Créez un objet Data pour stocker le graphe\n",
    "data = Data(x=x, edge_index=[edge_index_1, edge_index_2], y=y)\n",
    "\n",
    "train_dataset = [data, data]\n",
    "\n",
    "####################################################\"\"\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, channels, input_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.channels = channels\n",
    "        sc1 = 10\n",
    "        f1 = 20\n",
    "        sc2 = 10\n",
    "        self.GCN1 = nn.ModuleList([GCNConv(input_dim, sc1) for _ in range(channels)])\n",
    "        self.dense1 = nn.Linear(sc1*channels, f1)\n",
    "        self.GCN2 = nn.ModuleList([GCNConv(f1, sc2) for _ in range(channels)])\n",
    "        self.dense2 = nn.Linear(sc2*channels, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        nodes, edges = data.x, data.edge_index\n",
    "\n",
    "        # Appliquez les couches GCN avec une activation ReLU entre elles\n",
    "        x1 = []\n",
    "        for k in range(self.channels):\n",
    "            x = F.relu(self.GCN1[k](nodes, edges[k]))\n",
    "            x1.append(x)\n",
    "        x1_f = torch.cat(x1, dim=1)\n",
    "\n",
    "        f1 = F.relu(self.dense1(x1_f))\n",
    "\n",
    "        x2 = []\n",
    "        for k in range(self.channels):\n",
    "            x = F.relu(self.GCN2[k](f1, edges[k]))\n",
    "            x2.append(x)\n",
    "        x2_f = torch.cat(x2, dim=1)\n",
    "\n",
    "        x_out = self.dense2(x2_f)\n",
    "\n",
    "        return F.log_softmax(x_out, dim=1)\n",
    "    \n",
    "def train(model, )\n",
    "\n",
    "# Instanciez le modèle\n",
    "model = NodeClassifier(2,2)\n",
    "\n",
    "# Définissez la fonction de perte et l'optimiseur\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(train_dataset)\n",
    "# Entraînez le modèle\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        print(predicted)\n",
    "        total += data.y.size(0)\n",
    "        correct += (predicted == data.y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-6.1868e-05, -8.0344e-05, -4.0769e-05], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([0, 1, 0]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_p = Data(x=x, edge_index=[edge_index_1, edge_index_2])\n",
    "p = torch.max(model(d_p), 1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8120\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
